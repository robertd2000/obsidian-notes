Полный флоу тогда выглядит так, если учитывать все детали — датчики, Kafka, обогащение, Redis, PostgreSQL и отображение на карте.

В машине стоит GPS-трекер. Он раз в N секунд отправляет пакет: координаты, скорость, зажигание, топливо и т.д. Пакет уходит по HTTP/MQTT на входной сервис приёма телеметрии. Этот сервис почти ничего не считает — его задача быстро принять данные, провалидировать формат и положить событие в Kafka. Kafka здесь буфер и шина событий: она разгружает вход, выравнивает пики и позволяет нескольким сервисам читать поток независимо.

Дальше работает сервис обработки телеметрии. Он подписан на топики Kafka и получает поток координат. В этот момент ему нужно обогатить точку данными о транспортном средстве, водителе и компании. Он не делает REST-запросы в другие микросервисы на каждый пакет — вместо этого у него есть локальная копия справочников. Эти справочники приходят тоже через Kafka: сервисы ТС, водителей и CRM при любом изменении публикуют события `vehicle.updated`, `driver.updated`, `company.updated`. Сервис обработки слушает эти события и обновляет у себя два слоя хранения: Redis и PostgreSQL.

Redis используется как быстрый оперативный кэш — из него за миллисекунды берутся связи «машина → водитель → компания → тариф». PostgreSQL в сервисе обработки — это read-model, долговременная копия тех же справочников. Она нужна, чтобы при перезапуске сервиса быстро восстановить Redis, не ждать повторных событий и не ходить в чужие сервисы. Источником истины всё равно остаются их собственные базы, а здесь хранится только синхронизированная копия.

Когда из Kafka приходит GPS-пакет, сервис обработки берёт из Redis информацию о машине и водителе, объединяет данные и формирует уже обогащённое событие: координата + метаданные. После этого происходит несколько параллельных действий. Во-первых, обновляется «текущее состояние» машины — последняя точка, скорость, статус зажигания. Это состояние пишется в Redis (быстрое чтение для карты) и дублируется в PostgreSQL как актуальный слепок. Во-вторых, сырая или нормализованная телеметрия сохраняется в хранилище истории — обычно PostgreSQL с партициями или ClickHouse, если объёмы большие. В-третьих, сервис публикует событие о новой позиции в Kafka, уже в виде, удобном для потребителей.

Отдельно существует сервис real-time отображения / WebSocket-шлюз. Он не занимается обработкой телеметрии, его задача — держать тысячи постоянных соединений с браузерами. Он подписан на Kafka-топики с обновлениями позиций и параллельно имеет доступ к Redis с актуальным состоянием машин. Когда пользователь открывает карту, сначала идёт обычный HTTP-запрос за стартовым снимком состояния: фронт передаёт границы карты и фильтры (компании, группы ТС). WebSocket-сервис делает быстрый запрос в Redis/PostgreSQL read-модель и отдаёт список всех подходящих машин с их последними координатами. Это «начальное состояние».

После этого браузер открывает WebSocket-соединение. Все новые изменения позиций приходят уже потоковыми сообщениями. Как только сервис обработки опубликовал новое событие о машине в Kafka, WebSocket-сервис его получает, проверяет попадает ли машина в активные фильтры пользователя и отправляет маленький инкрементальный апдейт в браузер. То есть стартовое состояние — это массовый HTTP-ответ из Redis/PostgreSQL, а дальнейшие изменения — поток мелких событий через WebSocket.

История перемещений и отчёты работают отдельно. Когда пользователь запрашивает маршрут за период, фронт делает HTTP-запрос в сервис траекторий. Тот идёт не в Redis, а в историческое хранилище (PostgreSQL или ClickHouse), строит выборку точек и событий и возвращает уже агрегированный маршрут. Эти запросы не мешают real-time слою, потому что базы и сервисы разделены по типу нагрузки.

В итоге данные о ТС и водителях физически хранятся в нескольких местах одновременно: в своих «родных» сервисах как источник истины, в PostgreSQL сервиса обработки как read-модель и в Redis как быстрый кэш. Kafka служит только транспортом изменений между сервисами. За счёт этого обработка телеметрии не делает сетевых запросов на каждый пакет, карта получает обновления почти мгновенно, а система масштабируется горизонтально — можно добавлять инстансы обработки, WebSocket-шлюзы и сервисы отчётов независимо друг от друга.

## ingestion-сервис
Если смотреть на ingestion-сервис как на отдельный самостоятельный микросервис, то по сути это «ворота» всей системы телеметрии. Его задача — принять сырой поток данных от устройств и аккуратно, предсказуемо и максимально быстро передать его дальше, почти ничего не «думая» о бизнес-логике. Чем меньше он знает о мире, тем стабильнее он работает.

Представь, что у тебя тысячи трекеров в машинах. Каждый из них раз в N секунд шлёт пакет: координаты, скорость, направление, уровень топлива, иногда статус зажигания и кучу мелких флагов. Эти устройства могут слать данные по TCP, UDP, HTTP, MQTT — ingestion должен уметь принимать один или несколько протоколов, но внутри себя он всё равно приводит всё к единому формату. То есть на входе хаос, на выходе строго типизированный JSON/Protobuf/Avro объект вида «TelemetryMessage».

Дальше начинается самое важное — ingestion почти никогда не ходит в базы других сервисов и не делает синхронных запросов. Его задача — не блокироваться. Он живёт по принципу «принял → валидировал → положил в очередь». Максимум логики — это техническая валидация: есть ли deviceId, корректна ли широта/долгота, не мусор ли timestamp, не превышает ли скорость 1000 км/ч. Всё. Никаких «а существует ли такая машина в системе» — это не его зона ответственности.

Как он хранит данные. Обычно есть два уровня хранения.

Первый уровень — короткоживущий буфер или вообще его отсутствие. Если используется Kafka или другой брокер сообщений, то ingestion по сути не хранит данные у себя как бизнес-данные. Он держит их в памяти доли секунды и сразу публикует в топик, например `telemetry.raw`. Kafka и становится первым настоящим хранилищем. Это удобно тем, что ingestion можно горизонтально масштабировать бесконечно — он статeless.

Второй уровень — техническое долговременное хранение «сырья». Иногда делают отдельную PostgreSQL или ClickHouse таблицу `raw_telemetry`, куда складываются все пакеты. Это нужно не для онлайн-работы, а для аудита, расследований, переобработки, ML и прочего офлайна. Тогда ingestion после публикации в Kafka ещё и асинхронно пишет батчами в БД. Но это уже опционально — в реальном времени системе важнее брокер.

Как он общается с другими сервисами. Практически всегда через брокер сообщений. Он публикует события и никого напрямую не вызывает. Сервис обработки телеметрии подписывается на `telemetry.raw`, сервис алертов может подписаться на тот же поток, сервис аналитики — тоже. Ingestion ничего про них не знает. Это ключ к масштабируемости: добавился новый сервис — ingestion не меняется.

Иногда ingestion ещё публикует служебные события. Например, если устройство впервые появилось — событие `device.first_seen`. Если устройство долго молчит — можно генерировать `device.offline`. Но это уже тонкая грань, потому что такие вещи иногда выносят в отдельный monitoring-сервис, чтобы ingestion не разрастался логикой.

По хранению состояния самого ingestion-сервиса — его стараются делать максимально без состояния. Конфиги устройств, ключи авторизации, лимиты — обычно лежат в Redis или в отдельном конфигурационном сервисе. Сам ingestion держит это в кэше памяти, чтобы не ходить в сеть на каждый пакет. То есть Redis здесь — не бизнес-данные, а технический кэш.

Типичный флоу внутри него выглядит почти конвейером. Пришёл пакет → декодирование протокола → нормализация полей → техническая валидация → проставление serverTimestamp → публикация в Kafka → опциональная запись в raw-storage → логирование метрик. Всё это должно занимать миллисекунды, иначе при нагрузке он станет бутылочным горлышком.

Важно ещё понимать, что ingestion — это не место для сложной авторизации пользователей или проверки прав. Максимум — проверка токена устройства или API-ключа. Он работает на уровне «доверяем ли мы этому девайсу слать данные», а не «может ли этот водитель управлять этой машиной».

С точки зрения масштабирования это самый простой сервис в системе. Он масштабируется горизонтально, обычно за балансировщиком, не имеет локального состояния и может умирать и подниматься без последствий. Все критичные данные уже либо в Kafka, либо в Postgres.

По сути, если убрать ingestion, система теряет способность принимать данные. Если убрать всё остальное — данные всё равно будут приниматься и аккуратно складываться в очередь. И это хороший признак правильной архитектуры: вход всегда максимально тупой, быстрый и надёжный, а вся «умность» живёт дальше по пайплайну.