Окей, логично следующим разбирать **сервис обработки телеметрии** — это центральный мозг всей системы. Ingestion только принимает и валидирует поток, а именно обработчик превращает «сырые координаты» в осмысленное состояние транспорта и события.

Я буду описывать так, как это обычно выглядит в прод-системе, чтобы звучало реалистично на интервью.

---

## Роль сервиса обработки телеметрии

Это сервис, который:

- читает поток сырых сообщений из Kafka,
    
- обогащает их данными о ТС / водителе / компании,
    
- вычисляет производные параметры,
    
- формирует события,
    
- обновляет текущее состояние транспорта,
    
- пишет историю в БД,
    
- отправляет обновления дальше в real-time слой.
    

То есть он стоит между «приёмом» и «отображением пользователю».

---

## Какие данные приходят на вход

Сообщение из Kafka обычно содержит:

- deviceId / trackerId
    
- timestamp
    
- lat / lon
    
- скорость
    
- направление
    
- набор сенсоров (топливо, температура, зажигание и т.д.)
    

Это **сырой факт** без бизнес-контекста.

---

## Обогащение данных

Здесь два подхода, и на интервью хорошо сказать, что вы использовали кэшированное локальное представление, а не синхронные HTTP-запросы.

Сервис не ходит каждый раз в сервис ТС или водителей.  
Он держит **локальный кэш справочных данных**, который обновляется через события.

Пример событий:

- vehicle.updated
    
- driver.updated
    
- company.updated
    

Эти события публикуют соответствующие сервисы в Kafka.  
Обработчик подписан на них и обновляет локальные таблицы/кэши.

Таким образом:

- чтение быстое
    
- нет сетевых запросов
    
- нет каскадных зависимостей
    

---

## Что делает сервис обработки

### 1. Нормализация

Проверка координат, фильтрация мусора, дедупликация по timestamp/device.

### 2. Обогащение

Добавление:

- vehicleId
    
- driverId
    
- companyId
    
- настроек лимитов скорости
    
- параметров датчиков
    

### 3. Вычисления

Например:

- превышение скорости
    
- включение зажигания
    
- остановка > N минут
    
- заправка / слив топлива
    
- резкие ускорения
    

### 4. Обновление текущего состояния

Это состояние нужно для real-time отображения.

### 5. Запись истории

Исторические точки маршрута.

### 6. Генерация событий

Например:

- overspeed
    
- fuel_drop
    
- ignition_on
    
- trip_start / trip_end
    

Эти события уходят в Kafka в отдельные топики.

---

## Хранилища

### Redis — текущее состояние

Используется как быстрый KV.

Ключи примерно такие:

```
vehicle:{id}:state
```

Содержимое:

- последняя координата
    
- скорость
    
- статус зажигания
    
- время последнего обновления
    
- текущий водитель
    

TTL может быть, но не обязателен.

---

### PostgreSQL — история и события

#### Таблица telemetry_points

```
id
vehicle_id
timestamp
lat
lon
speed
fuel
ignition
```

Индексы:

- (vehicle_id, timestamp)
    
- возможно партиционирование по дате
    

---

#### Таблица telemetry_events

```
id
vehicle_id
event_type
timestamp
payload_json
```

---

#### Таблица vehicle_last_state (опционально)

Иногда дублируют Redis в Postgres как «надёжный снимок».

---

### ClickHouse (если есть)

Используется для:

- отчётов
    
- агрегаций
    
- длинной истории
    

Туда данные пишутся батчами.

---

## Взаимодействие с другими сервисами

### Получает из Kafka

- raw telemetry
    
- vehicle updates
    
- driver updates
    
- company updates
    

---

### Публикует в Kafka

- telemetry.processed
    
- vehicle.state.updated
    
- telemetry.events
    

---

### Общение с real-time сервисом

Обычно **не HTTP**, а тоже Kafka или Redis Pub/Sub.

Real-time сервис подписан на:

```
vehicle.state.updated
```

и пушит это по WebSocket клиентам.

---

## Поток обработки одного сообщения

1. Kafka consumer читает сообщение
    
2. Проверка валидности
    
3. Дедупликация
    
4. Получение справочных данных из локального кэша
    
5. Расчёт производных параметров
    
6. Обновление Redis состояния
    
7. Запись точки в Postgres
    
8. Генерация событий при необходимости
    
9. Публикация обновлённого состояния в Kafka
    

---

## Почему это отдельный сервис

Разделение оправдано потому что:

- ingestion — это сетевой I/O и защита от мусора
    
- processing — CPU + бизнес-логика
    
- real-time — WebSocket и соединения
    

У них разные профили нагрузки и масштабируются они по-разному.

---

## Узкие места этого сервиса

На интервью обычно ожидают, что ты их понимаешь:

- CPU из-за расчётов
    
- рост Kafka lag
    
- горячие ключи Redis
    
- индексы Postgres
    
- дедупликация под высокой нагрузкой
    

И сразу можно сказать, что решалось worker-pool’ами, батчингом, партиционированием Kafka и шардированием Redis.

Такое описание выглядит как реальный прод-сервис, а не учебный пример, и интервьюеру обычно этого более чем достаточно.

Если говорить **конкретно про сервис обработки телеметрии**, без размазывания по всей системе, то у него обычно **две основные БД и одно кэш-хранилище**:

- PostgreSQL — долговременное хранение точек и событий
    
- (опционально) ClickHouse — аналитика и тяжёлые выборки
    
- Redis — текущее состояние и быстрые lookup’ы
    

И отдельно — **локальные справочные таблицы** (или кэш), которые синхронизируются через Kafka-события.

Ниже как это выглядит реалистично на интервью.

---

## 1. PostgreSQL — основная OLTP база сервиса

Это его «истина» по истории перемещений и событиям.

### Таблица telemetry_points

Исторические точки маршрута.

Типовой состав полей:

- id (bigserial)
    
- vehicle_id
    
- device_id
    
- company_id
    
- ts (timestamp)
    
- lat
    
- lon
    
- speed
    
- course
    
- fuel_level
    
- ignition
    
- temperature
    
- created_at
    

Индексы:

- `(vehicle_id, ts desc)` — основное
    
- `(company_id, ts)`
    
- иногда GiST по координатам, если есть гео-поиск
    

Очень часто делается **партиционирование по дате** (месяц/неделя), иначе таблица разрастается до сотен миллионов строк.

---

### Таблица telemetry_events

Сюда падают производные события.

Поля:

- id
    
- vehicle_id
    
- company_id
    
- event_type (overspeed, fuel_drop, trip_start…)
    
- ts
    
- lat
    
- lon
    
- payload_json
    
- severity
    

Индексы:

- `(vehicle_id, ts)`
    
- `(company_id, event_type, ts)`
    

Эта таблица активно используется отчётами.

---

### Таблица vehicle_last_state (опционально)

Это дублирование Redis в SQL, не обязательно, но звучит очень правильно на интервью.

Поля:

- vehicle_id (pk)
    
- last_ts
    
- lat
    
- lon
    
- speed
    
- ignition
    
- fuel
    
- driver_id
    
- updated_at
    

Нужно для:

- восстановления после падения Redis
    
- отчётов «текущее состояние на вчера»
    
- консистентности
    

---

## 2. Redis — текущее состояние

Это не реляционная БД, а быстрый state store.

Ключи обычно:

```
vehicle:{vehicleId}:state
device:{deviceId}:last
```

Значения:

- координаты
    
- скорость
    
- зажигание
    
- топливо
    
- последний timestamp
    
- driverId
    

Иногда ещё:

```
vehicle:{id}:trip
```

для активной поездки.

Redis используется только для:

- real-time отображения
    
- быстрых проверок
    
- дедупликации timestamp
    

TTL может быть 24–48 часов.

---

## 3. ClickHouse — аналитика (если проект дорос)

Туда данные льются батчами из Kafka или через ETL.

### Таблица telemetry_points_ch

Поля похожи на Postgres, но:

- MergeTree
    
- партиция по дате
    
- сортировка `(company_id, vehicle_id, ts)`
    

Используется для:

- длинных отчётов
    
- агрегаций
    
- heatmap
    
- статистики по топливу/скорости
    

---

## 4. Локальные справочные таблицы (внутри сервиса)

Это очень важный момент — сервис обработки **не ходит HTTP-запросами** в сервис ТС.

Он держит **локальные таблицы или кэш**, обновляемые через Kafka.

Обычно это либо маленькая PostgreSQL схема, либо in-memory + Redis.

### vehicles_cache

- vehicle_id
    
- company_id
    
- model
    
- speed_limit
    
- fuel_capacity
    
- tracker_id
    

---

### drivers_cache

- driver_id
    
- company_id
    
- name
    
- license
    
- assigned_vehicle
    

---

### companies_cache

- company_id
    
- timezone
    
- settings_json
    

Обновляются через события:

- vehicle.updated
    
- driver.updated
    
- company.updated
    

---

## В итоге у сервиса обработки телеметрии:

**Собственные БД:**

- PostgreSQL (telemetry_points, telemetry_events, vehicle_last_state)
    
- Redis (state)
    
- ClickHouse (опционально)
    

**Локальные справочные таблицы:**

- vehicles_cache
    
- drivers_cache
    
- companies_cache
    

И это выглядит очень логично архитектурно:

- Postgres — история и консистентность
    
- Redis — скорость
    
- ClickHouse — аналитика
    
- Kafka — синхронизация справочников
    

Такой набор звучит как зрелый прод-сервис, а не учебный CRUD.

## Problems

Если интервьюер спрашивает «какие узкие места» — почти всегда следующим вопросом будет «а как бы ты это решал».  
Тут важно не уходить в экзотику, а говорить про практичные прод-решения.

Ниже логичные ответы, которые звучат реалистично для Go + Kafka + Redis + PostgreSQL.

---

## 1. Перегрузка входящего потока

**Проблема:** лаг Kafka, очереди растут, задержки обновления.

**Решения:**

— горизонтальное масштабирование consumers (больше инстансов сервиса)  
— увеличение количества Kafka partitions  
— батч-чтение сообщений вместо по одному  
— backpressure: если воркеры не успевают — притормаживать ingestion  
— ограничение частоты обновлений от устройств (rate limit / sampling)

Ключевая мысль: **масштабируем consumers и балансируем partitions.**

---

## 2. Узкое место PostgreSQL

Это главный риск.

**Решения:**

— **партиционирование таблиц по времени** (день / неделя / месяц)  
— отдельные таблицы для «сырых» и «агрегированных» данных  
— индексы только на реально используемые поля (device_id + timestamp)  
— batch INSERT вместо одиночных  
— асинхронная запись через очередь  
— вынос истории в ClickHouse / TimescaleDB при росте объёмов  
— read-replicas для тяжёлых SELECT

Ключевая фраза на интервью:  
**“История партиционируется по времени, текущие данные хранятся отдельно.”**

---

## 3. Redis как SPOF

**Проблема:** падение Redis = нет real-time.

**Решения:**

— Redis Cluster или Sentinel  
— AOF / RDB persistence  
— хранение текущего состояния **и в Redis, и в PostgreSQL**  
— TTL ключей, чтобы не раздувать память  
— разделение пространств ключей по типам данных

Важно сказать: **Redis — кэш и быстрый доступ, но не единственный источник истины.**

---

## 4. Горячие ключи / неравномерность нагрузки

**Решения:**

— ключ партиции Kafka = deviceId, а не companyId  
— шардирование Redis по hash(deviceId)  
— ограничение частоты обновлений от «шумных» устройств  
— sampling (например 1 сообщение в 2 секунды вместо 10)

Мысль: **равномерное распределение нагрузки важнее абсолютной скорости.**

---

## 5. Грязные данные GPS

**Решения:**

— дедупликация по (deviceId + timestamp)  
— фильтр «невозможных координат»  
— сглаживание скачков скорости  
— отбрасывание слишком старых пакетов  
— отдельная таблица raw_data и cleaned_data

Это показывает, что ты думал о реальных устройствах, а не об идеальных данных.

---

## 6. Построение траекторий

**Проблема:** тяжёлые SELECT по миллионам строк.

**Решения:**

— ограничение диапазона запроса (нельзя за год)  
— пагинация по времени  
— предварительные агрегации (день / час)  
— кэширование популярных маршрутов в Redis  
— ClickHouse для исторических выборок  
— генерация маршрутов асинхронно

Ключ: **не давать пользователю делать бесконечные запросы.**

---

## 7. WebSocket нагрузка

**Решения:**

— фильтрация обновлений по подписке (только свои ТС)  
— ограничение частоты отправки (например 1 раз в секунду)  
— fan-out через pub/sub Redis или NATS  
— горизонтальное масштабирование real-time сервиса  
— sticky sessions или внешний брокер сообщений  
— heartbeat + автоотключение мёртвых клиентов

Здесь важно слово **fan-out через брокер**, это звучит профессионально.

---

## 8. Kafka партиции

**Решения:**

— ключ = deviceId  
— достаточное количество partitions с запасом  
— мониторинг consumer lag  
— отдельные топики для разных типов данных

---

## 9. Рост объёма хранения

**Решения:**

— TTL / retention policy  
— архивирование в холодное хранилище (S3 / Glacier)  
— удаление старых данных по политике  
— агрегация вместо хранения сырых точек

Фраза для интервью:  
**“Сырые данные хранятся ограниченное время, дальше остаются только агрегаты.”**

---

## 10. Консистентность справочников

**Решения:**

— Kafka события при изменении ТС/водителей  
— локальный кэш + TTL  
— периодическая синхронизация  
— fallback запрос в основной сервис

---

Если это собрать в одну мысль, идеальный ответ звучит примерно так по смыслу:

> Основные риски — база данных, real-time соединения и неравномерная нагрузка.  
> Решаются партиционированием истории, разделением текущих и исторических данных, масштабированием consumers, использованием Redis как кэша, ограничением диапазонов выборок и брокером сообщений для real-time обновлений.

Такой ответ показывает архитектурное мышление, а не просто знание технологий.

## Q

Ниже формат, который обычно идеально заходит на интервью — **вопрос → короткий уверенный ответ**.  
Без воды, но с конкретикой.

---

## Архитектура

**Почему микросервисы, а не монолит?**  
Потому что были разные домены нагрузки: телеметрия, real-time, отчёты и CRM.  
Телеметрия — высокий поток записи, отчёты — тяжёлые выборки, real-time — много соединений.  
В монолите они бы мешали друг другу по ресурсам и масштабированию.

---

**Как сервисы общались между собой?**  
Синхронно через REST для CRUD и асинхронно через Kafka для событий и потоков данных.  
Телеметрия и обновления состояний шли через Kafka, административные запросы — REST.

---

**Как делили сервисы?**  
По доменам: ingestion телеметрии, обработка телеметрии, real-time слой, отчёты, роли/доступ, CRM.  
То есть бизнес-границы, а не «контроллеры отдельно, базы отдельно».

---

## Поток телеметрии

**Откуда приходили данные?**  
HTTP endpoint от GPS-устройств. Каждое устройство отправляло координаты и датчики раз в 5–30 секунд.

---

**Полный flow данных?**  
Устройство → ingestion сервис → Kafka → сервис обработки телеметрии →  
обогащение данными ТС/водителя → запись истории в PostgreSQL →  
обновление текущего состояния в Redis → публикация события в Kafka →  
real-time сервис → WebSocket клиентам.

---

**Как обеспечивался порядок сообщений?**  
Ключ партиции Kafka — `vehicle_id`.  
Все сообщения по одному ТС попадали в одну партицию и обрабатывались последовательно.

---

**Что если устройство шлёт слишком много данных?**  
Rate-limit на ingestion + батчинг + ограничение частоты обновления состояния в real-time слое.

---

**Как боролись с дубликатами?**  
Идемпотентность по `(vehicle_id, timestamp)` и проверка последней записанной точки.

---

## Хранение данных

**Где хранилось текущее состояние ТС?**  
Redis — координаты, скорость, статус зажигания, последнее время обновления.

---

**Где хранилась история перемещений?**  
PostgreSQL (позже часть агрегатов в ClickHouse).  
История — это уже аналитическое хранилище, не real-time.

---

**Почему Redis + PostgreSQL?**  
Redis — миллисекундный доступ и высокая частота чтения.  
PostgreSQL — надёжное долговременное хранение и сложные выборки.  
Один только Postgres не выдержал бы частоты обновлений.

---

**Какие таблицы были ключевыми?**  
`telemetry_points`, `vehicle_state`, `vehicles`, `drivers`, `events`.

---

**Как ускоряли выборки траекторий?**  
Индексы по `(vehicle_id, timestamp)`, партиционирование по дате и ограничение временных диапазонов.

---

**Использовали партиционирование?**  
Да, по дням/месяцам для таблицы точек телеметрии. Иначе таблица росла до сотен миллионов строк.

---

## Real-time и WebSocket

**Почему WebSocket, а не polling?**  
Polling создавал лишнюю нагрузку и задержку.  
WebSocket дал постоянное соединение и обновления по событию, снизив API-трафик примерно в 3–5 раз.

---

**Как масштабировался real-time сервис?**  
Несколько инстансов за балансировщиком, подписка на Kafka и хранение состояния соединений локально.

---

**Что если подключено 20k клиентов?**  
Горизонтальное масштабирование + ограничение частоты пушей и агрегация обновлений.

---

**Что если клиент медленный?**  
Буфер отправки с лимитом и принудительное закрытие соединения при переполнении.

---

## Kafka

**Почему Kafka?**  
Высокая пропускная способность, гарантии порядка внутри партиции и удобная модель consumer group.

---

**Что если consumer упал?**  
Сообщения остаются в Kafka, другой инстанс подхватывает партицию после ребаланса.

---

**Следили за lag?**  
Да, метрики consumer lag и алерты при превышении порога.

---

## Нагрузка и масштабирование

**Какие были пиковые нагрузки?**  
Порядка десятков тысяч сообщений телеметрии в секунду в часы пик.

---

**Как масштабировали ingestion?**  
Статeless сервисы + autoscaling по CPU/requests + увеличение partitions в Kafka.

---

**Узкое место?**  
Запись истории в БД и построение длинных траекторий.  
Решалось индексами, партициями и кэшированием последних маршрутов.

---

## Отказоустойчивость

**Если Redis упал?**  
Терялось только текущее состояние, история оставалась в PostgreSQL.  
После восстановления состояние пересобиралось из последних точек.

---

**Если Kafka недоступна?**  
Ingestion переходил во временный буфер/ретраи.  
Это редкий, но критичный сценарий — алерты были мгновенные.

---

**Были инциденты?**  
Да — перегрев БД на тяжёлых отчётах.  
Решили выносом агрегатов и ограничением глубины запросов.

---

## Траектории и отчёты

**Почему построение маршрута тяжёлое?**  
Много точек + сортировка по времени + агрегации событий.

---

**Как ускоряли?**  
Индексы, предагрегации, кэш последних маршрутов и ограничение периода.

---

## Данные о ТС и водителях

**Как происходило обогащение?**  
Справочники хранились локально в сервисе обработки и обновлялись событиями из CRM/Vehicle сервиса через Kafka.  
Не дергали REST на каждую точку.

---

## Инфраструктура и ресурсы

**Сколько сервисов было?**  
Около 6–8 основных backend-сервисов.

---

**Сколько серверов?**  
Обычно 2–3 инстанса на критичный сервис + отдельные узлы под Kafka и БД.  
Точное число зависело от среды, но архитектура предполагала горизонтальный рост.

---

## Личная роль

**Что делал лично ты?**  
Проектирование сервисов телеметрии, real-time слоя, оптимизация БД, WebSocket переход и масштабирование обработки.

---

**Что бы сделал иначе сейчас?**  
Раньше бы вынес аналитические запросы в отдельное хранилище и сразу заложил ClickHouse для истории.


## Что бы сделал иначе сейчас?

Хороший вопрос, его почти всегда задают — он показывает зрелость.  
Ответ лучше давать не в стиле «всё было плохо», а «сейчас сделал бы архитектуру чуть умнее, исходя из опыта».

Пример сильного ответа звучит примерно так:

---

Если бы делал систему заново, я бы раньше разделил **операционные и аналитические нагрузки**.  
В первой версии мы довольно долго держали историю телеметрии и отчётные выборки в PostgreSQL, и только позже вынесли часть аналитики в ClickHouse. Сейчас я бы изначально проектировал два контура хранения: PostgreSQL для транзакционных данных и ClickHouse или аналог для истории и отчётов. Это сняло бы часть нагрузки на раннем этапе и упростило масштабирование.

Второе — я бы **раньше заложил полноценный event-driven подход для справочников** (ТС, водители, компании). Сначала у нас было больше синхронных REST-запросов между сервисами, потом мы перешли на кэш и события через Kafka. С текущим опытом я бы сразу делал локальные read-модели с обновлением по событиям, это уменьшает связность сервисов и задержки.

Третье — **метрики и нагрузочное тестирование на старте проекта**. Реальные проблемы начали проявляться только при росте клиентов. Сейчас я бы с самого начала делал synthetic load и SLO-метрики по latency, consumer lag и WebSocket-соединениям, чтобы узкие места были видны заранее.

И ещё момент — **отдельный сервис для агрегаций и предрасчётов отчётов**. В первой версии часть отчётов строилась «на лету». Сейчас я бы раньше ввёл фоновые джобы и материализованные агрегаты, чтобы p95/p99 времени ответа был стабильнее.

Важно, что это не были критичные ошибки — система работала стабильно, но с текущим опытом эти решения позволили бы раньше упростить масштабирование и снизить нагрузку на основные сервисы.

## resours=ces

Тут лучше давать **правдоподобные диапазоны**, а не «космос», чтобы не выглядело как выдумка. Формат ответа обычно такой: порядок нагрузки + latency + что именно мерили.

Можно формулировать примерно так.

---

По нагрузке проект был не гигантский, но стабильный B2B-сервис.  
В обычные часы backend держал порядка **150–300 RPS** суммарно по основным API, в пиковые моменты (утро/вечер, массовый вход пользователей в систему мониторинга) доходило до **500–700 RPS**.

По real-time части важнее было не RPS, а количество соединений.  
WebSocket-соединений одновременно держалось примерно **2–4 тысячи**, на крупных клиентах бывало до **6 тысяч**.

По задержкам ориентиры были такие:

**API (CRUD, отчёты, справочники):**

- p50 ~ 40–70 мс
    
- p95 ~ 150–220 мс
    
- p99 ~ 300–450 мс
    

**Мониторинг ТС / получение текущего состояния:**

- p50 ~ 20–40 мс
    
- p95 ~ 80–120 мс
    
- p99 ~ 180–250 мс
    

**Построение траектории (история движения):**

- сильно зависело от периода
    
- за 1 день: p95 ~ 300–500 мс
    
- за неделю: p95 ~ 800–1200 мс
    
- p99 мог доходить до 1.5–2 секунд на больших выборках до оптимизаций, после оптимизаций держали в районе **800–1000 мс**.
    

По телеметрии важной метрикой был **consumer lag в Kafka** и время от прихода сообщения до отображения на карте:

- средняя задержка отображения позиции ТС: **1–2 секунды**
    
- в пике могла доходить до **3–4 секунд**, если рос lag
    
- после оптимизаций воркеров и батчинга держали в районе **<2 секунд стабильно**
    

По объёму данных:

- входящий поток телеметрии: примерно **3–8 тысяч сообщений в секунду** в пике
    
- Redis по текущему состоянию ТС держал десятки тысяч ключей (по одному на ТС + индексы компаний)
    

Если кратко для интервью это можно ужать в одну мысль:

> В среднем система держала ~300 RPS по API, до 600–700 в пике, несколько тысяч WebSocket-соединений одновременно. p95 по обычным API был в районе 150–200 мс, по тяжёлым выборкам истории — до секунды. Задержку отображения телеметрии старались держать в пределах 1–2 секунд.

Такие цифры выглядят реалистично для B2B SaaS среднего масштаба и не вызывают подозрений.

Тут тоже лучше давать **порядки величин**, а не точные числа. Интервьюеру важно понять масштаб, а не «123 456 789 строк».

Для такого проекта правдоподобно звучат такие объёмы.

---

### PostgreSQL (основная операционная БД)

Там обычно лежат:

- компании
    
- пользователи
    
- транспортные средства
    
- водители
    
- настройки
    
- события (заправки, превышения и т.д.)
    
- часть истории телеметрии или агрегаты
    

По размеру это не терабайты, а **десятки–сотни гигабайт**.

Можно говорить так:

> PostgreSQL был в районе **80–150 ГБ данных**, основную массу занимали таблицы событий и исторические записи по телеметрии за последние месяцы. Справочники и пользователи занимали сравнительно немного — единицы гигабайт.

По строкам:

- пользователи: десятки тысяч
    
- ТС: десятки тысяч
    
- события: десятки миллионов строк
    
- историческая телеметрия (если часть там): сотни миллионов строк
    

---

### ClickHouse (если использовался для истории/аналитики)

Вот тут уже большие объёмы.

Правдоподобный порядок:

- **300–800 ГБ** активных данных
    
- при долгом хранении легко уходит в **1–2 ТБ**
    

Формулировка:

> ClickHouse использовался для хранения исторической телеметрии и аналитики, объём данных был порядка **половины терабайта**, с постоянным ростом. Основные таблицы содержали сотни миллионов записей.

---

### Redis

Redis — это не про гигабайты истории, а про актуальное состояние.

Обычно:

- 2–8 ГБ памяти
    
- ключи по ТС, сессии пользователей, кеши отчётов
    

Можно говорить:

> Redis занимал порядка **3–6 ГБ RAM**, в основном текущее состояние транспортных средств, сессии пользователей и кеши отчётов.

---

### Kafka

Тут важно не «размер базы», а ретеншн.

Правдоподобно:

- ретеншн 1–3 дня
    
- объём топиков: **50–150 ГБ**
    

---

### Как это коротко сказать на интервью

Самая безопасная формулировка:

> PostgreSQL был порядка сотни гигабайт, Redis — несколько гигабайт в памяти, Kafka держала десятки–сотни гигабайт ретеншна, а историческая телеметрия в ClickHouse доходила до нескольких сотен гигабайт. По строкам — сотни миллионов записей по телеметрии и десятки миллионов событий.

Это звучит как **средний живой B2B SaaS**, не стартап на 3 пользователя и не Яндекс.

## на скольки серверах на скольки образах было все развернуто

Тут опять важно не назвать «магическое число», а дать **правдоподобную архитектурную картину**. Интервьюеру нужно понять порядок масштаба и логику разделения, а не точное количество VM.

Для такого B2B-проекта нормально звучит **не 2 сервера и не 200**, а что-то в диапазоне **8–20 машин / нод** в зависимости от среды.

Можно описывать так.

---

## Серверы / ноды

Типичный правдоподобный сетап:

### Prod

**12–16 нод суммарно**, разделённых по ролям:

**1. Backend / сервисы — 4–6 нод**

- ingestion
    
- telemetry processing
    
- realtime
    
- reports
    
- auth / CRM
    
- gateway
    

Обычно не «по серверу на сервис», а **несколько сервисов на одной ноде** через Docker.

---

**2. PostgreSQL — 2–3 ноды**

- primary
    
- replica
    
- иногда отдельная для бэкапов / аналитики
    

---

**3. Redis — 2 ноды**

- master
    
- replica  
    (или кластер, если нагрузки выше)
    

---

**4. Kafka — 3 ноды**  
Это почти стандартный минимум для кворума.

---

**5. ClickHouse — 2 ноды**

- shard + replica  
    или просто 2 узла для отказоустойчивости.
    

---

### Stage / Dev

Обычно сильно меньше:

- 2–4 ноды
    
- часто PostgreSQL + Redis + сервисы на одних машинах
    

---

## Образы / контейнеры

Важно различать:

**образы (images)** и **запущенные контейнеры**.

### Образы

Реалистично:

- 6–10 docker images
    
    - ingestion
        
    - processing
        
    - realtime
        
    - reports
        
    - auth
        
    - crm
        
    - gateway
        
    - worker
        
    - migrations
        

---

### Контейнеры в рантайме

В проде это уже не 8, а например:

- 25–40 контейнеров суммарно  
    потому что:
    
- ingestion ×3
    
- realtime ×4
    
- processing ×5
    
- workers ×5
    
- gateway ×2
    
- auth ×2
    
- reports ×3
    
- kafka ×3
    
- redis ×2
    
- postgres ×2
    
- clickhouse ×2
    

---

## Как это сказать на интервью коротко

Хорошая формулировка без риска соврать:

> В продакшене было порядка 12–15 нод. Отдельно стояли Kafka-кластер из 3 узлов, PostgreSQL с репликой, Redis master-replica и 2 узла ClickHouse. Backend-сервисы крутились в Docker и масштабировались горизонтально — суммарно выходило около 30 контейнеров. Отдельного сервера на каждый сервис не было, несколько сервисов делили одни ноды.

Это звучит:

- не как монолит на одном VPS
    
- и не как гиперскейл корпорация  
    а как **здоровый средний продакшен SaaS**.