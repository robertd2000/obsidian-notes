# 1.  Горутина
Отличный вопрос! Давайте разберемся подробно.

**Горутина (Goroutine)** — это легковесный поток, управляемый средой выполнения Go (Go runtime), а не операционной системой напрямую.

Проще говоря, **это функция, которая выполняется конкурентно с другими функциями в том же адресном пространстве**. Концепция очень похожа на потоки (threads), но с ключевыми отличиями.

### Ключевые характеристики горутин

1.  **Легковесность**
    *   Запуск горутины требует всего несколько килобайт памяти, и их стек может динамически расти и сжиматься.
    *   Для сравнения: поток ОС по умолчанию может занимать 1-2 МБ. Это позволяет вам запускать *десятки тысяч* горутин одновременно без особых проблем.

2.  **Управление в пространстве пользователя**
    *   Планировщик Go (scheduler), который является частью рантайма, сам управляет горутинами, а не ОС. Он работает поверх потоков ОС (обычно их количество равно количеству ядер CPU).
    *   Планировщик распределяет горутины по этим потокам и переключается между ними в моменты, когда горутина блокируется (например, ждет I/O - чтение файла, сети и т.д.) или явно уступает место (`runtime.Gosched()`).

3.  **Простой запуск**
    *   Чтобы запустить функцию как горутину, нужно просто поставить перед ее вызовом ключевое слово `go`.
    *   Не нужно работать с низкоуровневыми примитивами, как при создании потоков в других языках.

### Простой пример

```go
package main

import (
    "fmt"
    "time"
)

// Обычная функция
func say(s string) {
    for i := 0; i < 3; i++ {
        time.Sleep(100 * time.Millisecond)
        fmt.Println(s)
    }
}

func main() {
    // Запускаем функцию say в отдельной горутине
    go say("горутина") // <- вот она, магия!

    // Вызываем функцию say в главной горутине (main)
    say("главная")

    // Небольшая задержка, чтобы горутина успела выполниться до выхода из main
    time.Sleep(500 * time.Millisecond)
}
```

**Возможный вывод:**
```
главная
горутина
горутина
главная
главная
горутина
```

Вы видите, что вывод перемешан — это доказывает, что две функции (`say("главная")` и `say("горутина")`) выполнялись *конкурентно*.

### Аналогия

Представьте, что вы — шеф-повар на кухне (ваша программа — это `main`).

*   **Поток ОС** — это как нанять еще одного повара. Это дорого (память), и их не может быть много (ограничения ОС).
*   **Горутина** — это как поставить кастрюлю на плиту и пойти резать овощи, пока она закипает. Вы, один повар, делаете несколько дел "параллельно", переключаясь между ними. Вы не ждете, пока вода закипит, стоя над кастрюлей.

### Для чего используются горутины?

Горутины — идеальный инструмент для задач, где много времени тратится на ожидание (I/O-bound задачи):

*   Обработка HTTP-запросов в веб-сервере (каждый запрос обрабатывается в своей горутине).
*   Работа с базами данных.
*   Чтение и запись файлов.
*   Общение по сети (сокеты).
*   Фоновые задачи (отправка email, генерация отчетов).

### Важные моменты и связь с каналами (Channels)

1.  **main ждет горутины**: Если главная функция `main` завершится, все запущенные в ней горутины будут немедленно остановлены. Поэтому часто нужно синхронизировать их завершение.
2.  **Синхронизация и обмен данными**: Горутины выполняются в одном адресном пространстве, поэтому они имеют доступ к общей памяти. Однако прямое чтение/запись в общую память из разных горутин небезопасно. Для безопасной коммуникации и синхронизации в Go используются **каналы (channels)**.

**Пример с каналом для синхронизации:**

```go
package main

import "fmt"

func worker(done chan bool) {
    fmt.Print("работаю...")
    // ...выполняем работу
    fmt.Println("готово!")
    // Отправляем сигнал о завершении в канал
    done <- true
}

func main() {
    // Создаем канал для синхронизации
    done := make(chan bool, 1)

    // Запускаем горутину
    go worker(done)

    // Ожидаем сигнал из канала от горутины
    <-done // Главная функция заблокируется здесь, пока не придет значение.
}
```

### Итог

**Горутина** — это фундаментальный строительный блок для конкурентного программирования в Go. Это:
*   **Легковесный поток** (дешевле потоков ОС).
*   **Простой в использовании** (достаточно ключевого слова `go`).
*   **Эффективно управляемый** (планировщиком Go).
*   **Идеальный инструмент** для создания высокопроизводительных и масштабируемых concurrent-приложений.

Сочетание **горутин** (для выполнения) и **каналов** (для коммуникации) образует мощную и элегантную модель конкурентности в Go, известную как **CSP (Communicating Sequential Processes)**.

# 2. Планировщик го
Отличный вопрос! **Планировщик Go (Go Scheduler)** — это мозг конкурентности в Go, который делает горутины такими эффективными. Давайте разберем его работу подробно.

## Что такое планировщик Go?

Это часть runtime Go, которая отвечает за распределение горутин по потокам операционной системы (OS threads) и управление их выполнением. Он работает в пространстве пользователя, а не ядра ОС.

## Архитектура: Модель M:P:G

Планировщик построен вокруг трех ключевых сущностей:

### **G (Goroutine)** - Горутина
- Представляет саму горутину
- Содержит стек, состояние выполнения, указатель на функцию

### **M (Machine) - Поток ОС**
- Абстракция над потоком ядра ОС
- Именно "M" выполняет код горутин
- Связан с ядром процессора

### **P (Processor) - Процессор**
- Виртуальный процессор, ресурс для выполнения горутин
- Каждый P имеет локальную очередь горутин
- Связывает M и G

```go
// Упрощенное представление архитектуры
[P1] -> Локальная очередь: [G1, G2, G3]
     -> M1 -> Выполняет G1
     
[P2] -> Локальная очередь: [G4, G5]
     -> M2 -> Выполняет G4

[Глобальная очередь] -> [G6, G7, G8]
```

## Как работает планировщик

### 1. **Запуск горутины**
```go
func main() {
    go myFunction() // Создается G, помещается в локальную очередь P
}
```

### 2. **Распределение работы**
- Каждый P обслуживает свою локальную очередь горутин
- Если локальная очередь пуста, P "крадет" работу из других P или глобальной очереди

### 3. **Перепланирование (scheduling)**
Планировщик вмешивается в нескольких случаях:

#### **a) Системные вызовы (блокирующие)**
```go
file, err := os.Open("file.txt") // Блокирующий системный вызов
```
- Когда горутина делает блокирующий системный вызов:
  - P "отсоединяется" от текущего M
  - Создается новый M или берется из пула для выполнения других горутин
  - Когда системный вызов завершен, G возвращается в очередь

#### **b) Сетевые операции**
```go
resp, err := http.Get("https://example.com")
```
- Go использует асинхронный ввод/вывод через netpoller
- G ставится в очередь ожидания, P освобождается для других задач
- Когда данные готовы, G возвращается в очередь выполнения

#### **c) Каналы и синхронизация**
```go
data := <-ch // Ожидание данных из канала
```
- G блокируется, P переключается на другую горутину

#### **d) Явное переключение**
```go
runtime.Gosched() // Явно уступить процессорное время
```

#### **e) Вытеснение (preemption)**
- Начиная с Go 1.14, планировщик может вытеснять долго работающие горутины
- Предотвращает "голодание" других горутин

## Преимущества такой архитектуры

### **1. Эффективность**
- Переключение между горутинами дешевле, чем между потоками ОС
- Нет переключения контекста ядра

### **2. Масштабируемость**
- Можно запускать миллионы горутин
- Автоматическое использование всех ядер CPU

### **3. Интеграция с вводом/выводом**
- Netpoller эффективно обрабатывает тысячи сетевых соединений

## Практический пример

```go
package main

import (
    "fmt"
    "runtime"
    "time"
)

func worker(id int, results chan<- int) {
    fmt.Printf("Worker %d started\n", id)
    time.Sleep(time.Second) // Имитация работы
    results <- id * 2
}

func main() {
    // Показываем количество процессоров
    fmt.Printf("Количество CPU: %d\n", runtime.NumCPU())
    fmt.Printf("Количество горутин: %d\n", runtime.NumGoroutine())
    
    results := make(chan int, 10)
    
    // Запускаем 10 горутин
    for i := 0; i < 10; i++ {
        go worker(i, results)
    }
    
    fmt.Printf("После запуска горутин: %d\n", runtime.NumGoroutine())
    
    // Собираем результаты
    for i := 0; i < 10; i++ {
        result := <-results
        fmt.Printf("Результат: %d\n", result)
    }
    
    fmt.Printf("После завершения: %d\n", runtime.NumGoroutine())
}
```

## Настройка планировщика

```go
// Установка максимального количества потоков ОС
runtime.GOMAXPROCS(4)

// Показать текущий GOMAXPROCS
fmt.Println(runtime.GOMAXPROCS(0))

// Принудительно запустить сборщик мусора
runtime.GC()

// Явно уступить процессор
runtime.Gosched()
```

## Итог

**Планировщик Go** — это интеллектуальная система, которая:
- Автоматически распределяет работу по всем ядрам CPU
- Эффективно управляет тысячами горутин
- Минимизирует блокировки потоков ОС
- Обеспечивает высокую производительность I/O-bound приложений

Благодаря планировщику программист может писать простой конкурентный код, не задумываясь о сложностях низкоуровневого управления потоками.

При запуске Go-программы среда выполнения Go спрашивает машину (виртуальную или физическую) сколько потоков операционной системы может работать параллельно. Этот вопрос основывается на количестве ядер, доступных программе. Для каждого потока, который может работать параллельно, среда выполнения создает поток операционной системы (M) и прикрепляет его к структуре данных которая представляет собой логический процессор (P) внутри программы. Эти P и M представляют собой вычислительные мощности или контекст исполнения для выполнения программы Go.

Также создается начальная горутина (G) для управления выполнением инструкций на выбранном M/P. Точно так же, как M управляет выполнением инструкций на аппаратном обеспечении, G управляет выполнением инструкций на М. Это создает новый уровень абстракции над операционной системой, но переносит управление выполнением на уровень приложений.

[![](https://tour.ardanlabs.com/tour/eng/static/img/gor1.png)](https://tour.ardanlabs.com/tour/eng/static/img/gor1.png)

Поскольку планировщик Go располагается поверх планировщика операционной системы, важно иметь некоторое семантическое представление о планировщике операционной системы и ограничениях, которые он накладывает на планировщик Go и приложения.

Задача планировщика операционной системы - создавать иллюзию того, что несколько частей работы выполняются одновременно. Даже если это физически невозможно. Это требует некоторых компромиссов при проектировании планировщика. Прежде чем продолжить, важно дать определение некоторым словам.

**Работа:** Набор инструкций для выполнения в запущенном приложении. Это выполняется потоками, и приложение может иметь от 1 до многих потоков.

**Поток:** Путь выполнения, который планируется и выполняется. Потоки отвечают за выполнение инструкций на аппаратном оборудовании.

**Состояния** **Потока:** Поток может находиться в одном из трех состояний: Выполнение, Готовность к выполнению или ожидание. Выполнение означает, что поток выполняет назначенные ему инструкции на аппаратном уровне, для этого на M ставится G. Готовность к выполнению означает, что поток хочет получить время на аппаратном обеспечении для выполнения назначенных ему инструкций и находится в очереди на выполнение. Ожидание означает, что поток ждет чего-то, прежде чем сможет продолжить свою работу. Ожидающие потоки не являются заботой планировщика.

**Конкурентность:** Это означает неопределенное выполнение вне очереди. Другими словами, набор инструкций, который должен быть выполнен в указанном порядке, выполняется в другом, неопределенном порядке, но полностью. Ключевым моментом является то, что результат выполнения полного набора инструкций в любом неопределенном порядке дает один и тот же результат. Вы скажете, что работа может выполняться параллельно, когда порядок ее выполнения не имеет значения, пока вся работа завершена.

**Параллельность:** Это означает одновременное выполнение множества действий. Чтобы это стало возможным, необходима возможность физического выполнения двух или более потоков операционной системы одновременно на аппаратном обеспечении.

**Работа,** **с** **Загрузкой** **CPU:** Это работа, которая не приводит к естественному переходу потока в состояние ожидания. Вычисление чисел Фибоначчи будет считаться работой, которая загружает CPU.

**Работа** **с** *Загрузкой **I/O:** Это работа, которая заставляет поток естественным образом переходить в состояние ожидания. состояние ожидания. Получение данных с различных URL будет считаться работой, которая загружает I/O.

**Синхронизация:** Когда двум или более горутинам необходимо получить доступ к одному и тому же месту памяти они должны быть синхронизированы и выполняться по очереди. Если синхронизация не выполняется, и хотя бы одна горутина выполняет запись, то может возникнуть гонка данных. Гонки данных являются причиной повреждения данных, баги, которые трудно обнаружить.

**Оркестрация:** Когда двум или более горутинам нужно подать друг другу сигнал, с данными или без данных, требуется механика оркестрации. Если оркестрация не происходит, гарантии одновременного выполнения и завершения работы будут упущены. Это может привести к всевозможным ошибкам, связанным с повреждением данных.

# 3. Почему горутины быстрые
Отличный вопрос! **Горутины быстрые** благодаря комбинации архитектурных решений, которые работают вместе. Давайте разберем по полочкам.

## Основные причины скорости горутин

### 1. **Легковесные стеки**

**Потоки ОС:** Фиксированный стек 1-8 МБ
```go
// Поток ОС: 1MB стека сразу
// 1000 потоков = 1000 * 1MB = 1GB памяти только на стеки!
```

**Горутины:** Динамический стек 2-8 КБ начально
```go
// Горутина: начинается с 2KB
func main() {
    for i := 0; i < 100000; i++ {
        go worker(i) // 100000 * 2KB = 200MB - приемлемо!
    }
}
```
- **Стек растет/сжимается** по мере необходимости
- **Копирование стека** при росте вместо выделения новых страниц

### 2. **Планировщик в пространстве пользователя**

```go
package main

import "runtime"

func fastSwitch() {
    // Переключение происходит БЕЗ системного вызова
    // Планировщик Go сам решает когда переключаться
    ch := make(chan int)
    
    go func() {
        ch <- 42  // Точка переключения - дешевая!
    }()
    
    <-ch  // Точка переключения - дешевая!
}
```

**Преимущества:**
- **Нет системных вызовов** для переключения
- **Нет перехода в ядро** ОС
- **Минимальные накладные расходы** на сохранение контекста

### 3. **Кооперативная многозадачность с вытеснением**

```go
func cooperativeExample() {
    go func() {
        // Горутина САМА уступает управление в определенных точках:
        // - При операциях с каналами
        // - При системных вызовах  
        // - При вызове runtime.Gosched()
        // - При работе с sync пакетом
    }()
}
```

**Точки вытеснения:**
```go
ch <- data           // Отправка в канал
<-ch                 // Получение из канала
time.Sleep()         // Сон
http.Get()           // Сетевой I/O
file.Read()          // Файловый I/O  
runtime.Gosched()    // Явное переключение
mutex.Lock()         // Синхронизация
```

### 4. **Эффективная работа с I/O через netpoller**

```go
package main

import (
    "net/http"
    "time"
)

func efficientIO() {
    for i := 0; i < 10000; i++ {
        go func(id int) {
            // Вместо блокировки потока ОС:
            resp, _ := http.Get("https://api.example.com/data")
            // Горутина приостанавливается, поток ОС освобождается
            // для выполнения других горутин!
            _ = resp
        }(i)
    }
}
```

**Netpoller magic:**
- **Неблокирующий I/O** на уровне ОС
- **Горутины "спят"** пока данные не готовы
- **Потоки ОС не блокируются** - обслуживают другие горутины

### 5. **Work-stealing планировщик**

```go
// Планировщик эффективно распределяет работу:
// [P1] -> [G1, G2, G3]  // Занят
// [P2] -> []             // Свободен - "крадет" G3 у P1!
// [P3] -> [G4, G5]       // Занят
```

**Принципы work-stealing:**
- Каждый P (процессор) имеет локальную очередь
- Если у P нет работы - крадет у других P
- **Минимизирует простои** процессоров
- **Автоматическая балансировка** нагрузки

## Сравнительный бенчмарк

```go
package main

import (
    "sync"
    "testing"
    "time"
)

// Бенчмарк горутин
func BenchmarkGoroutines(b *testing.B) {
    var wg sync.WaitGroup
    for i := 0; i < b.N; i++ {
        wg.Add(1)
        go func(id int) {
            time.Sleep(1 * time.Microsecond)
            wg.Done()
        }(i)
    }
    wg.Wait()
}

// Результаты показывают: 
// 100000 операций за несколько миллисекунд
```

## Числовое сравнение

| Операция | Потоки ОС | Горутины |
|----------|-----------|----------|
| **Создание** | 10-30 µs | 0.1-0.3 µs |
| **Переключение** | 1-1.5 µs | 0.1-0.3 µs |
| **Память** | 1-8 MB | 2-8 KB |
| **Максимум** | ~10,000 | ~1,000,000 |

## Практический пример производительности

```go
package main

import (
    "fmt"
    "net/http"
    "sync"
    "time"
)

func main() {
    start := time.Now()
    var wg sync.WaitGroup
    urls := []string{
        "https://httpbin.org/delay/1",
        "https://httpbin.org/delay/1", 
        "https://httpbin.org/delay/1",
    }
    
    // Все запросы выполняются КОНКУРЕНТНО
    for _, url := range urls {
        wg.Add(1)
        go func(u string) {
            defer wg.Done()
            resp, _ := http.Get(u)
            if resp != nil {
                resp.Body.Close()
            }
            fmt.Printf("Completed: %s\n", u)
        }(url)
    }
    
    wg.Wait()
    fmt.Printf("Total time: %v (instead of ~3s)\n", time.Since(start))
    // Вывод: Total time: ~1.1s - все запросы параллельно!
}
```

## Итог: почему горутины быстрые

1. **🚀 Легковесные** - маленькие стеки, быстрая инициализация
2. **⚡ Пользовательский планировщик** - нет дорогих системных вызовов  
3. **🔄 Умное переключение** - кооперативная модель с вытеснением
4. **📡 Эффективный I/O** - netpoller не блокирует потоки ОС
5. **🎯 Work-stealing** - автоматическая балансировка нагрузки
6. **💰 Локальные очереди** - минимум конкуренции за ресурсы

**Результат:** Go может обрабатывать миллионы одновременных соединений на одном сервере, что делает его идеальным для современных высоконагруженных приложений и микросервисов.
# 4. Внутреннее устройство горутины

### Объект горутины 

_Длина объекта горутины составляет порядка 70 строк. Позвольте мне удалить комментарии и навести небольшой порядок:_

```go
type g struct {
	stack          stack
	stackguard0    uintptr
	stackguard1    uintptr
	_panic         *_panic
	_defer         *_defer
	m              *m
	sched          gobuf
	syscallsp      uintptr
	syscallpc      uintptr
	stktopsp       uintptr
	param          unsafe.Pointer
	atomicstatus   uint32
	stackLock      uint32
	goid           int64
	schedlink      guintptr
	waitsince      int64
	waitreason     waitReason
	preempt        bool
	preemptStop    bool
	preemptShrink  bool
	asyncSafePoint bool
	paniconfault   bool
	gcscandone     bool
	throwsplit     bool
	activeStackChans bool
	raceignore     int8
	sysblocktraced bool
	sysexitticks   int64
	traceseq       uint64
	tracelastp     puintptr
	lockedm        muintptr
	sig            uint32
	writebuf       []byte
	sigcode0       uintptr
	sigcode1       uintptr
	sigpc          uintptr
	gopc           uintptr
	ancestors      *[]ancestorInfo
	startpc        uintptr
	racectx        uintptr
	waiting        *sudog
	cgoCtxt        []uintptr
	labels         unsafe.Pointer
	timer          *timer
	selectDone     uint32
	gcAssistBytes  int64
}
```

Вот и все!

Попробуем подсчитать суммарный размер; `uintptr` весит 64 бит, т.е. 8 байт в нашей архитектуре, так же как и `int64`. Логические значения имеют размер в 1 байт, а слайс - это просто указатель плюс два инта.

Есть более сложные типы, такие как `timer` (~70 байт), `_panic` (~40 байт) или `_defer` (~100 байт), но в целом я насчитал около 600 байт.

Хм, это кажется немного подозрительным… Откуда взялось пресловутое значение «2 кб»?

Давайте подробнее рассмотрим первое поле структуры…

### Стек горутины

Первое поле структуры `g` имеет тип `stack`.

```go
type g struct {
	// Параметры стека.
	// stack описывает фактическую память стека: [stack.lo, stack.hi).
	// stackguard0 - указатель стека, сравниваемый по мере роста стека Go.
	// stackguard1 - указатель стека, сравниваемый по мере роста стека C.
	// ...
	stack       stack   // смещение, известное runtime/cgo
	stackguard0 uintptr // смещение, известное liblink
	stackguard1 uintptr // смещение, известное liblink
}
```

Сам стек представляет собой не что иное, как два значения, обозначающих его начало и конец.

```go
type stack struct {
	lo uintptr
	hi uintptr
}
```

К этому времени вы, вероятно, зададитесь вопросом: _«А каков же размер этого стека?»_, или уже догадаетесь, что 2 килобайта относятся к этому стеку!

**_Горутина_** [**_стартует_**](https://github.com/golang/go/blob/f296b7a6f045325a230f77e9bda1470b1270f817/src/runtime/proc.go#L3410) **_с минимального размера стека в_** [**_2 килобайта_**](https://github.com/golang/go/blob/f296b7a6f045325a230f77e9bda1470b1270f817/src/runtime/stack.go#L72)**_, который увеличивается и уменьшается по мере необходимости без риска когда-либо закончиться._**

[Эта](https://dave.cheney.net/2013/06/02/why-is-a-goroutines-stack-infinite) отличная статья Дэйва Чейни более подробно объясняет, как это работает. По сути, перед выполнением любой функции Go проверяет, доступен ли объем стека, необходимый для функции, которую он собирается выполнить; если нет, то выполняется вызов [runtime.morestack,](https://github.com/golang/go/blob/f296b7a6f045325a230f77e9bda1470b1270f817/src/runtime/asm_amd64.s#L407) который выделяет новую страницу, и только после этого выполняется функция. Наконец, когда эта функция завершается, ее возвращаемые аргументы копируются обратно в исходный фрейм стека, а все невостребованное пространство стека высвобождается.

Хотя минимальный размер стека определен как 2048 байтов, рантайм Go также не позволяет горутинам превышать [максимальный размер стека](https://github.com/golang/go/blob/f296b7a6f045325a230f77e9bda1470b1270f817/src/runtime/stack.go#L1031); этот максимум зависит от архитектуры и составляет [1 ГБ для 64-разрядных систем и 250 МБ для 32-разрядных](https://github.com/golang/go/blob/f296b7a6f045325a230f77e9bda1470b1270f817/src/runtime/proc.go#L120) систем.

Если этот предел достигнут, будет выполнен вызов [runtime.abort](https://github.com/golang/go/blob/f296b7a6f045325a230f77e9bda1470b1270f817/src/runtime/asm_amd64.s#L450). Превысить этого размер очень просто с помощью рекурсивной функции; все, что вам нужно сделать, это

```go
package main

func foo(i int) int {
	if i < 1e8 {
		return foo(i + 1)
	}
	return -1
}

func main() {
	foo(0)
}
```

И мы видим, что приложение паникует, стек больше не может расти и выполняется вышеупомянутый runtime.abort .

```
$ go run exceed-stack.goruntime: goroutine stack exceeds 1000000000-byte limitfatal error: stack overflowruntime stack:runtime.throw(0x1071ce1, 0xe)	/usr/local/go/src/runtime/panic.go:774 +0x72runtime.newstack()	/usr/local/go/src/runtime/stack.go:1046 +0x6e9runtime.morestack()	/usr/local/go/src/runtime/asm_amd64.s:449 +0x8fgoroutine 1 [running]:main.foo(0xffffdf, 0x0)......
```

# 5. Отличие потока от горутины
Отличный вопрос! Давайте детально разберем **отличия потоков от горутин** с техническими деталями и примерами.

## Основные различия

| Характеристика | Потоки ОС (Threads) | Горутины (Goroutines) |
|----------------|-------------------|---------------------|
| **Управление** | Ядро ОС | Go runtime (пространство пользователя) |
| **Память** | 1-8 МБ (фиксированный стек) | 2-8 КБ (динамический стек) |
| **Переключение** | 1000-1500 ns (дорогое) | 200-500 ns (дешевое) |
| **Планировщик** | Планировщик ОС | Планировщик Go |
| **Максимум** | 1000-10000 | 1000000+ |
| **Создание** | Медленное (~10-30 µs) | Быстрое (~0.1-0.3 µs) |

## Технические отличия

### 1. **Управление и планирование**

**Потоки ОС:**
```go
import (
    "fmt"
    "sync"
    "time"
)

func threadLikeWork() {
    var wg sync.WaitGroup
    
    // Каждый вызов создает поток ОС (через планировщик ОС)
    for i := 0; i < 1000; i++ {
        wg.Add(1)
        go func(id int) {  // Здесь "go" создает поток ОС
            defer wg.Done()
            time.Sleep(time.Millisecond)
            fmt.Printf("Thread %d\n", id)
        }(i)
    }
    wg.Wait()
    // Проблема: 1000 потоков ОС - это тяжело для системы!
}
```

**Горутины:**
```go
func goroutineWork() {
    var wg sync.WaitGroup
    
    // Легковесные горутины управляются Go runtime
    for i := 0; i < 100000; i++ {
        wg.Add(1)
        go func(id int) {  // Легковесная горутина
            defer wg.Done()
            time.Sleep(time.Millisecond)
            fmt.Printf("Goroutine %d\n", id)
        }(i)
    }
    wg.Wait()
    // 100000 горутин - не проблема для Go!
}
```

### 2. **Память и стеки**

**Потоки ОС:**
```go
// Каждый поток получает фиксированный стек 1-8 МБ
// 1000 потоков = 1000 * 1MB = 1 ГБ памяти только на стеки!

func threadMemoryDemo() {
    // Создание многих потоков быстро исчерпывает память
    for i := 0; i < 5000; i++ {
        go heavyOperation() // Может привести к OOM
    }
}
```

**Горутины:**
```go
// Начальный стек: 2 КБ, растет динамически
// 100000 горутин = 100000 * 2KB = 200 МБ (приемлемо!)

func goroutineMemoryDemo() {
    // Можно создавать сотни тысяч горутин
    for i := 0; i < 500000; i++ {
        go lightOperation() // Без проблем с памятью
    }
}
```

### 3. **Модель выполнения**

**Потоки ОС - Вытесняющая многозадачность:**
```go
func osThreadExample() {
    // Потоки вытесняются планировщиком ОС:
    // - По исчерпанию кванта времени
    // - По приоритетам
    // - По системным прерываниям
    
    go cpuIntensiveTask() // Может "монополизировать" ядро
    go ioIntensiveTask()  // Может "голодать" из-за CPU-bound задачи
}
```

**Горутины - Кооперативная многозадачность с вытеснением:**
```go
func goroutineExample() {
    // Горутины уступают управление в точках:
    go func() {
        // 1. Операции с каналами
        ch <- data
        <-ch
        
        // 2. Системные вызовы
        http.Get("...")
        file.Read()
        
        // 3. Sleep
        time.Sleep(...)
        
        // 4. Явное переключение
        runtime.Gosched()
        
        // 5. Сборка мусора
    }()
}
```

## Практические примеры различий

### Пример 1: Создание множества задач

**С потоками ОС (проблематично):**
```go
func withOSThreads() {
    start := time.Now()
    var wg sync.WaitGroup
    
    // Проблема при большом количестве
    for i := 0; i < 50000; i++ {
        wg.Add(1)
        go func(id int) {
            defer wg.Done()
            // Имитация работы
            time.Sleep(1 * time.Microsecond)
        }(i)
    }
    
    wg.Wait()
    fmt.Printf("Threads time: %v\n", time.Since(start))
    // Вероятно: out of memory или очень медленно
}
```

**С горутинами (эффективно):**
```go
func withGoroutines() {
    start := time.Now()
    var wg sync.WaitGroup
    
    // Эффективно даже для больших чисел
    for i := 0; i < 500000; i++ {
        wg.Add(1)
        go func(id int) {
            defer wg.Done()
            // Имитация работы
            time.Sleep(1 * time.Microsecond)
        }(i)
    }
    
    wg.Wait()
    fmt.Printf("Goroutines time: %v\n", time.Since(start))
    // Быстро и без проблем с памятью
}
```

### Пример 2: Работа с I/O

**Потоки ОС блокируются:**
```go
func blockingIOWithThreads() {
    // Каждый сетевой запрос блокирует поток ОС
    for i := 0; i < 1000; i++ {
        go func() {
            resp, _ := http.Get("https://api.example.com")
            // Поток ОС БЛОКИРУЕТСЯ на время запроса!
            _ = resp
        }()
    }
    // Требуется 1000 потоков ОС для 1000 concurrent запросов
}
```

**Горутины не блокируют потоки ОС:**
```go
func nonBlockingIOWithGoroutines() {
    // Go использует netpoller для асинхронного I/O
    for i := 0; i < 100000; i++ {
        go func() {
            resp, _ := http.Get("https://api.example.com")
            // Горутина приостанавливается, но поток ОС НЕ блокируется
            // Поток ОС может выполнять другие горутины!
            _ = resp
        }()
    }
    // Достаточно нескольких потоков ОС для 100000 concurrent запросов
}
```

## Сравнительная таблица производительности

| Операция | Потоки ОС | Горутины | Выигрыш |
|----------|-----------|----------|---------|
| **Создание** | ~20 µs | ~0.3 µs | 60x |
| **Переключение** | ~1.2 µs | ~0.2 µs | 6x |
| **Память/экземпляр** | ~1 MB | ~2 KB | 500x |
| **Concurrent connections** | ~10k | ~1M | 100x |

## Когда что использовать?

### **Используйте потоки ОС когда:**
- Работаете с C/C++ библиотеками, требующими настоящих потоков
- Выполняете CPU-intensive задачи, которые не должны быть прерваны
- Требуется низкоуровневый контроль над выполнением

### **Используйте горутины когда:**
- Строите высоконагруженные сетевые сервисы
- Нужно обрабатывать тысячи одновременных соединений
- Выполняете I/O-bound задачи (сеть, файлы, БД)
- Хотите простую модель конкурентности

## Пример из реального мира

**Веб-сервер на потоках:**
```go
// Псевдокод - традиционный подход
func traditionalWebServer() {
    for {
        conn := acceptConnection()
        go handleConnection(conn) // Создает поток ОС
        // Ограничение: ~10k одновременных подключений
    }
}
```

**Веб-сервер на горутинах:**
```go
// Реальный Go подход
func goWebServer() {
    http.HandleFunc("/", handler)
    http.ListenAndServe(":8080", nil)
    // Каждый HTTP запрос - отдельная горутина
    // Возможно обслуживать 1M+ одновременных подключений
}

func handler(w http.ResponseWriter, r *http.Request) {
    // Каждый вызов - в отдельной горутине
    // Легковесная, эффективная обработка
}
```

## Итог

**Ключевые отличия:**

1. **💾 Память** - горутины на порядки экономнее
2. **⚡ Производительность** - быстрое создание и переключение  
3. **🎯 Управление** - планировщик Go эффективнее ОС
4. **📡 Масштабируемость** - миллионы горутин vs тысячи потоков
5. **🔄 Модель** - кооперативная vs вытесняющая многозадачность

**Горутины** - это абстракция более высокого уровня, которая делает конкурентное программирование простым, безопасным и эффективным, скрывая сложность управления потоками ОС.
# 6. Воровство у горутин
Отличный вопрос! **Work stealing (воровство задач)** — это ключевой механизм планировщика Go для балансировки нагрузки и эффективного использования ресурсов.

## Что такое work stealing?

**Work stealing** — это алгоритм, при котором **свободные процессоры (P) "крадут" горутины из очередей занятых процессоров**, вместо того чтобы простаивать.

## Архитектура воровства в Go

```go
// runtime/proc.go - упрощенная структура
type p struct {
    // Локальная очередь горутин
    runq [256]guintptr
    runqhead uint32
    runqtail uint32
    
    runnext guintptr // Высокоприоритетная следующая горутина
}
```

## Как работает воровство

### Базовая схема:

```
[P1] -> [G1, G2, G3, G4]  // Занят, много работы
[P2] -> [G5]              // Занят, мало работы  
[P3] -> []                // Свободен - ИЩЕТ работу!
[P4] -> [G6, G7]          // Занят

// P3 "крадет" G4 из P1 чтобы не простаивать
```

### Реализация в коде:

```go
// runtime/proc.go
func findrunnable() (gp *g, inheritTime bool) {
    // Попытка украсть работу у других P
    if gp := runqsteal(_p_, allp[enum.position()]); gp != nil {
        return gp, false
    }
}
```

## Детальный процесс воровства

### 1. **Когда происходит воровство?**

```go
func scheduler() {
    for {
        // 1. Проверяем локальную очередь
        if gp, inheritTime := runqget(_p_); gp != nil {
            execute(gp, inheritTime) // Выполняем свою работу
        }
        
        // 2. Локальная очередь пуста - пытаемся украсть
        if gp := runqsteal(_p_, otherP); gp != nil {
            execute(gp, false) // Выполняем украденную работу
        }
        
        // 3. Другие стратегии если воровство не удалось
        // ...
    }
}
```

### 2. **Стратегии воровства**

```go
// runtime/proc.go
func runqsteal(pp, otherp *p) *g {
    // Пытаемся украсть половину горутин из очереди другого P
    n := runqgrab(otherp, &pp.runq, t)
    if n == 0 {
        return nil
    }
    return runqget(pp)
}

func runqgrab(pp *p, batch *[256]guintptr, t int) int {
    // Берем до половины горутин из очереди
    n := pp.runqtail - pp.runqhead
    n = n - n/2 // Берем половину
    
    // Атомарно перемещаем горутины в свою очередь
    // ...
    return n
}
```

## Практическая демонстрация

### Пример 1: Демонстрация воровства

```go
package main

import (
    "fmt"
    "runtime"
    "sync"
    "time"
)

func worker(id int, wg *sync.WaitGroup) {
    defer wg.Done()
    fmt.Printf("Worker %d executed on CPU %d\n", id, getCPUID())
    time.Sleep(10 * time.Millisecond)
}

func getCPUID() int {
    // Получаем ID текущего P (процессора)
    return runtime.GetProcessorID()
}

func main() {
    runtime.GOMAXPROCS(4) // 4 процессора
    
    var wg sync.WaitGroup
    
    // Создаем асимметричную нагрузку
    // Больше работы на первом процессоре
    for i := 0; i < 100; i++ {
        wg.Add(1)
        if i < 80 {
            // 80% работы отправляем в "тяжелую" горутину
            go heavyWorker(i, &wg)
        } else {
            go worker(i, &wg)
        }
    }
    
    wg.Wait()
    fmt.Println("All workers completed")
}

func heavyWorker(id int, wg *sync.WaitGroup) {
    defer wg.Done()
    // Имитация тяжелой работы
    sum := 0
    for i := 0; i < 1000000; i++ {
        sum += i
    }
    fmt.Printf("Heavy worker %d on CPU %d\n", id, getCPUID())
}
```

### Пример 2: Визуализация распределения

```go
package main

import (
    "fmt"
    "runtime"
    "sync"
    "sync/atomic"
)

var cpuWorkers [4]int64 // Счетчики работы по CPU

func distributedWorker(id int, wg *sync.WaitGroup) {
    defer wg.Done()
    
    // Определяем на каком P выполняемся
    pID := getCurrentP()
    atomic.AddInt64(&cpuWorkers[pID], 1)
    
    // Имитация работы
    for i := 0; i < 10000; i++ {
        // some work
    }
}

func getCurrentP() int {
    // В реальном коде нужно использовать runtime/internal/sys
    // Здесь упрощенная демонстрация
    return runtime.GOMAXPROCS(0) - 1 // упрощение
}

func main() {
    runtime.GOMAXPROCS(4)
    
    var wg sync.WaitGroup
    
    // Запускаем много горутин
    for i := 0; i < 1000; i++ {
        wg.Add(1)
        go distributedWorker(i, &wg)
    }
    
    wg.Wait()
    
    // Выводим статистику распределения
    fmt.Println("Work distribution across CPUs:")
    total := int64(0)
    for i, count := range cpuWorkers {
        fmt.Printf("CPU %d: %d tasks\n", i, count)
        total += count
    }
    fmt.Printf("Total: %d tasks\n", total)
    // Благодаря work stealing нагрузка будет распределена равномерно
}
```

## Типы воровства в Go

### 1. **Воровство из локальных очередей**

```go
// P1 украдет у P2 из runq
[P1] -> []              // Пустая очередь
[P2] -> [G1, G2, G3, G4] // Полная очередь

// P1 крадет G3 и G4
```

### 2. **Воровство runnext**

```go
// runnext - высокоприоритетная горутина
[P1] -> runnext: G5, runq: [G1, G2]
[P2] -> []  // Свободен

// P2 сначала пытается украсть runnext (G5), затем из runq
```

### 3. **Воровство из глобальной очереди**

```go
// Если локальные очереди пусты
// Процессоры периодически проверяют глобальную очередь

global runq -> [G100, G101, G102...]

[P1] -> []  // Проверяет глобальную очередь и берет G100
```

## Преимущества work stealing

### 1. **Балансировка нагрузки**

```go
func loadBalancingExample() {
    // Без work stealing:
    // CPU1: [G1, G2, G3, G4, G5]  // Перегружен
    // CPU2: []                     // Простаивает
    
    // С work stealing:
    // CPU1: [G1, G2, G3]          // Баланс
    // CPU2: [G4, G5]              // Украдено у CPU1
}
```

### 2. **Уменьшение простоя**

```go
func noIdleTime() {
    // Свободные процессоры немедленно находят работу
    // вместо ожидания в планировщике ОС
}
```

### 3. **Локальность данных**

```go
func dataLocality() {
    // Каждый P старается выполнять свои горутины
    // Но при простое эффективно перераспределяет работу
}
```

## Реальный пример: HTTP сервер

```go
package main

import (
    "fmt"
    "net/http"
    "runtime"
    "sync/atomic"
    "time"
)

var requestCount int64

func handler(w http.ResponseWriter, r *http.Request) {
    atomic.AddInt64(&requestCount, 1)
    
    // Имитация обработки запроса
    time.Sleep(10 * time.Millisecond)
    
    // Work stealing гарантирует, что все CPU будут использоваться
    // даже если запросы приходят неравномерно
    cpu := runtime.GOMAXPROCS(0)
    fmt.Fprintf(w, "Request %d handled (CPUs: %d)", 
        atomic.LoadInt64(&requestCount), cpu)
}

func main() {
    http.HandleFunc("/", handler)
    
    fmt.Println("Server starting on :8080")
    fmt.Println("Work stealing will balance load across all CPUs")
    
    http.ListenAndServe(":8080", nil)
}
```

## Мониторинг work stealing

```go
package main

import (
    "fmt"
    "runtime"
    "time"
)

func monitorStealing() {
    for {
        var m runtime.MemStats
        runtime.ReadMemStats(&m)
        
        // Выводим информацию о горутинах
        fmt.Printf("Goroutines: %d, CPUs: %d\n", 
            runtime.NumGoroutine(), 
            runtime.GOMAXPROCS(0))
        
        time.Sleep(2 * time.Second)
    }
}

func main() {
    go monitorStealing()
    
    // Создаем асимметричную нагрузку
    for i := 0; i < 1000; i++ {
        go func(id int) {
            // Длительная работа
            for {
                time.Sleep(time.Second)
            }
        }(i)
    }
    
    select {} // Бесконечное ожидание
}
```

## Итог

**Work stealing в Go обеспечивает:**

1. **⚖️ Автоматическую балансировку** нагрузки между процессорами
2. **🚫 Отсутствие простоя** - свободные CPU сразу находят работу
3. **📈 Высокую утилизацию** всех ядер процессора
4. **🎯 Адаптивность** к изменяющейся нагрузке
5. **🏃‍♂️ Минимальные накладные расходы** на синхронизацию

Этот механизм — одна из ключевых причин, почему Go показывает такую высокую производительность в highly concurrent сценариях, эффективно используя все доступные ресурсы CPU без ручной настройки со стороны разработчика.
# 7. Локальная очередь процесса в планировщике
Отличный вопрос! **Локальная очередь процесса (P)** — это ключевой компонент планировщика Go, обеспечивающий высокую производительность.

## Структура локальной очереди

```go
// runtime/runtime2.go
type p struct {
    // Основная локальная очередь - кольцевой буфер на 256 горутин
    runqhead uint32           // Голова очереди (чтение)
    runqtail uint32           // Хвост очереди (запись)  
    runq     [256]guintptr    // Кольцевой буфер горутин
    
    // Высокоприоритетная горутина (выполняется следующей)
    runnext guintptr
    
    // Ссылка на поток OS
    m        muintptr
    
    // Статус процессора
    status   uint32  // _Pidle, _Prunning, _Psyscall
}
```

## Принцип работы локальной очереди

### Базовая схема:
```
Локальная очередь P (runq):
[head] -> [G1][G2][G3][G4][G5] ... [G256] <- [tail]
           ↑                           ↑
        чтение (runqget)           запись (runqput)
```

## Операции с локальной очередью

### 1. **Добавление горутины в очередь**

```go
// runtime/proc.go
func runqput(_p_ *p, gp *g, next bool) {
    if next {
        // 1. Высокий приоритет - в runnext
        oldnext := _p_.runnext
        _p_.runnext = gp
        if oldnext != 0 {
            // Старую горутину из runnext перемещаем в общую очередь
            gp = oldnext
        } else {
            return
        }
    }
    
    // 2. Добавление в основную очередь (runq)
    for {
        h := atomic.LoadAcq(&_p_.runqhead)
        t := _p_.runqtail
        
        if t-h < uint32(len(_p_.runq)) {
            // Есть место в очереди
            _p_.runq[t%uint32(len(_p_.runq))] = gp
            atomic.StoreRel(&_p_.runqtail, t+1)
            return
        }
        
        // Очередь полна - перемещаем половину в глобальную очередь
        if runqputslow(_p_, gp, h, t) {
            return
        }
    }
}
```

### 2. **Извлечение горутины из очереди**

```go
func runqget(_p_ *p) (gp *g, inheritTime bool) {
    // 1. Сначала проверяем runnext (высокий приоритет)
    for {
        next := _p_.runnext
        if next == 0 {
            break
        }
        if _p_.runnext.cas(next, 0) {
            return next.ptr(), true
        }
    }
    
    // 2. Затем основную очередь
    for {
        h := atomic.LoadAcq(&_p_.runqhead)
        t := _p_.runqtail
        if t == h {
            return nil, false // Очередь пуста
        }
        
        gp := _p_.runq[h%uint32(len(_p_.runq))].ptr()
        if atomic.CasRel(&_p_.runqhead, h, h+1) {
            return gp, false
        }
    }
}
```

## Практическая демонстрация

### Пример 1: Наполнение локальной очереди

```go
package main

import (
    "fmt"
    "runtime"
    "sync"
    "time"
)

func worker(id int, wg *sync.WaitGroup) {
    defer wg.Done()
    fmt.Printf("Worker %d started\n", id)
    time.Sleep(10 * time.Millisecond)
}

func main() {
    // Устанавливаем 1 CPU для наглядности
    runtime.GOMAXPROCS(1)
    
    var wg sync.WaitGroup
    
    fmt.Printf("Initial goroutines: %d\n", runtime.NumGoroutine())
    
    // Запускаем больше горутин, чем размер локальной очереди (256)
    for i := 0; i < 300; i++ {
        wg.Add(1)
        go worker(i, &wg)
        
        // Даем время для планировщика
        if i%50 == 0 {
            fmt.Printf("After %d goroutines: %d in system\n", 
                i, runtime.NumGoroutine())
            time.Sleep(1 * time.Millisecond)
        }
    }
    
    wg.Wait()
    fmt.Printf("Final goroutines: %d\n", runtime.NumGoroutine())
}
```

### Пример 2: Демонстрация runnext

```go
package main

import (
    "fmt"
    "runtime"
    "sync"
    "time"
)

func highPriorityWorker(id int) {
    fmt.Printf("HIGH PRIORITY %d executed\n", id)
}

func normalWorker(id int, wg *sync.WaitGroup) {
    defer wg.Done()
    fmt.Printf("Normal worker %d\n", id)
    time.Sleep(time.Millisecond)
}

func main() {
    runtime.GOMAXPROCS(1)
    
    var wg sync.WaitGroup
    
    // Заполняем локальную очередь
    for i := 0; i < 10; i++ {
        wg.Add(1)
        go normalWorker(i, &wg)
    }
    
    // Даем время заполнить очередь
    time.Sleep(time.Millisecond)
    
    // Горутина, которая должна получить высокий приоритет
    // Она попадет в runnext и выполнится СРАЗУ после текущей
    go highPriorityWorker(999)
    
    time.Sleep(time.Millisecond)
    
    wg.Wait()
}
```

## Размеры и ограничения

### Стандартные размеры:
```go
const (
    runqSize = 256          // Размер локальной очереди
    runqBatch = 32          // Количество горутин для воровства
)

// Когда очередь заполняется:
func runqputslow(_p_ *p, gp *g, h, t uint32) bool {
    var batch [len(_p_.runq)/2 + 1]*g
    
    // Берем первую половину из локальной очереди
    n := t - h
    n = n / 2
    
    for i := uint32(0); i < n; i++ {
        batch[i] = _p_.runq[(h+i)%uint32(len(_p_.runq))].ptr()
    }
    
    if !atomic.CasRel(&_p_.runqhead, h, h+n) {
        return false
    }
    
    // Перемещаем половину в глобальную очередь
    batch[n] = gp
    for i := uint32(0); i < n+1; i++ {
        batch[i].schedlink = guintptr(0)
    }
    
    globrunqputbatch(&batch[0], n+1)
    return true
}
```

## Преимущества локальных очередей

### 1. **Локальность кэша процессора**

```go
func cacheLocalityExample() {
    // Каждый P работает со своей очередью
    // Данные горутин остаются "горячими" в кэше CPU
    // Минимизация cache misses
}
```

### 2. **Снижение конкуренции**

```go
func lowContention() {
    // Каждый P работает со своей очередью без блокировок
    // Нет конкуренции за глобальную очередь
    // Атомарные операции только при переполнении/воровстве
}
```

### 3. **Быстрые операции**

```go
func fastOperations() {
    // runqget/runqput - очень быстрые
    // Обычно 1 атомарная операция на операцию
    // Локальные данные = быстрый доступ
}
```

## Взаимодействие с глобальной очередью

### Переполнение локальной очереди:

```go
func overflowHandling() {
    // Когда локальная очередь заполнена (256 горутин):
    // 1. Берется первая половина (128 горутин)
    // 2. Перемещается в глобальную очередь
    // 3. Новая горутина добавляется в локальную очередь
    
    // Схема:
    // Локальная [G1..G256] -> Переполнение -> 
    // Глобальная [G1..G128] + Локальная [G129..G256, newG]
}
```

## Мониторинг локальных очередей

### Пример мониторинга:

```go
package main

import (
    "fmt"
    "runtime"
    "time"
)

func queueMonitor() {
    for {
        // Получаем количество процессоров
        procs := runtime.GOMAXPROCS(0)
        
        fmt.Printf("=== Queue Monitor ===\n")
        fmt.Printf("CPUs: %d, Goroutines: %d\n", 
            procs, runtime.NumGoroutine())
        
        // В реальном коде здесь была бы внутренняя статистика
        // о размерах локальных очередей
        
        time.Sleep(2 * time.Second)
    }
}

func main() {
    go queueMonitor()
    
    // Создаем нагрузку
    for i := 0; i < 500; i++ {
        go func(id int) {
            time.Sleep(time.Second * 10)
        }(i)
    }
    
    select {}
}
```

## Реальный сценарий: Веб-сервер

```go
package main

import (
    "fmt"
    "net/http"
    "runtime"
    "time"
)

func handler(w http.ResponseWriter, r *http.Request) {
    // Каждый HTTP запрос - отдельная горутина
    start := time.Now()
    
    // Имитация обработки
    time.Sleep(10 * time.Millisecond)
    
    // Информация о выполнении
    fmt.Fprintf(w, "Request processed in %v\n", time.Since(start))
}

func main() {
    http.HandleFunc("/", handler)
    
    fmt.Printf("Server starting with %d CPUs\n", runtime.GOMAXPROCS(0))
    fmt.Printf("Each CPU has local queue of %d goroutines\n", 256)
    
    // Локальные очереди позволяют эффективно обрабатывать
    // тысячи одновременных запросов
    http.ListenAndServe(":8080", nil)
}
```

## Итог

**Локальная очередь процесса P — это:**

1. **🏃‍♂️ Высокопроизводительный кольцевой буфер** на 256 горутин
2. **🎯 Runnext слот** для высокоприоритетных задач
3. **💾 Кэш-дружественная** структура данных
4. **🚫 Бесконфликтный доступ** в нормальных условиях
5. **⚡ Автоматическое перераспределение** при переполнении

Эта архитектура позволяет Go эффективно обрабатывать миллионы горутин с минимальными накладными расходами на синхронизацию, что критически важно для высоконагруженных concurrent приложений.
# 8. Глобальная очередь
Отличный вопрос! **Глобальная очередь** — это важный компонент планировщика Go, который работает вместе с локальными очередями для балансировки нагрузки.

## Структура глобальной очереди

```go
// runtime/runtime2.go
type schedt struct {
    // Глобальная очередь горутин
    runq     gQueue    // Очередь горутин
    runqsize int32     // Размер очереди
    
    // Lock для глобальной очереди
    runqlock mutex     // Защищает доступ к глобальной очереди
    
    // Статистика
    npidle   uint32    // Количество idle P
    nmspinning uint32  // Количество spinning M
}
```

## Принцип работы глобальной очереди

### Базовая схема:
```
Глобальная очередь (sched.runq):
[G100] ↔ [G101] ↔ [G102] ↔ [G103] ↔ ... ↔ [G999]

Локальные очереди P:
[P1] -> [G1, G2, G3]    [P2] -> [G4, G5]
[P3] -> [] (пусто)      [P4] -> [G6, G7]

// P3 забирает горутины из глобальной очереди
```

## Операции с глобальной очередью

### 1. **Добавление горутин в глобальную очередь**

```go
// runtime/proc.go
func globrunqput(gp *g) {
    // Блокировка глобальной очереди
    lock(&sched.runqlock)
    
    // Добавление горутины в конец глобальной очереди
    sched.runq.pushBack(gp)
    sched.runqsize++
    
    unlock(&sched.runqlock)
}

// Пакетное добавление
func globrunqputbatch(batch []*g, n int32) {
    if n == 0 {
        return
    }
    
    lock(&sched.runqlock)
    for i := int32(0); i < n; i++ {
        sched.runq.pushBack(batch[i])
    }
    sched.runqsize += n
    unlock(&sched.runqlock)
}
```

### 2. **Извлечение горутин из глобальной очереди**

```go
func globrunqget(_p_ *p, max int32) *g {
    // Проверяем, есть ли горутины в глобальной очереди
    if sched.runqsize == 0 {
        return nil
    }
    
    lock(&sched.runqlock)
    
    // Вычисляем сколько взять (не более 1/2 от размера глобальной очереди)
    n := sched.runqsize / runtime.GOMAXPROCS(0) + 1
    if n > sched.runqsize {
        n = sched.runqsize
    }
    if n > max {
        n = max
    }
    if n == 0 {
        unlock(&sched.runqlock)
        return nil
    }
    
    // Берем горутины из глобальной очереди
    gp := sched.runq.pop() // Первая горутина
    n--
    sched.runqsize--
    
    // Помещаем остальные в локальную очередь P
    for n > 0 {
        gp1 := sched.runq.pop()
        sched.runqsize--
        runqput(_p_, gp1, false)
        n--
    }
    
    unlock(&sched.runqlock)
    return gp
}
```

## Когда используется глобальная очередь

### 1. **Переполнение локальной очереди**

```go
// runtime/proc.go
func runqputslow(_p_ *p, gp *g, h, t uint32) bool {
    // Когда локальная очередь переполнена (256 горутин)
    // Берем первую половину и перемещаем в глобальную очередь
    
    var batch [len(_p_.runq)/2 + 1]*g
    n := t - h
    n = n / 2
    
    // Формируем batch из первой половины локальной очереди
    for i := uint32(0); i < n; i++ {
        batch[i] = _p_.runq[(h+i)%uint32(len(_p_.runq))].ptr()
    }
    
    if !atomic.CasRel(&_p_.runqhead, h, h+n) {
        return false
    }
    
    // Добавляем новую горутину и перемещаем в глобальную очередь
    batch[n] = gp
    globrunqputbatch(&batch[0], n+1)
    return true
}
```

### 2. **Создание новых горутин**

```go
// runtime/proc.go
func newproc1(fn *funcval, argp unsafe.Pointer, narg int32, callergp *g, callerpc uintptr) {
    // Создание новой горутины
    newg := gfget(_p_)
    
    if randomizeScheduler && randomEnabled && debug.schedrace == 0 {
        delay := uint32(fastrand())
        if delay%10 == 0 { // 10% chance попасть в глобальную очередь
            lock(&sched.runqlock)
            globrunqput(newg)
            unlock(&sched.runqlock)
            return
        }
    }
    
    // Обычно попадает в локальную очередь
    runqput(_p_, newg, true)
}
```

## Практическая демонстрация

### Пример 1: Переполнение локальной очереди

```go
package main

import (
    "fmt"
    "runtime"
    "sync"
    "time"
)

func worker(id int, wg *sync.WaitGroup) {
    defer wg.Done()
    time.Sleep(time.Microsecond * 100)
    if id%100 == 0 {
        fmt.Printf("Worker %d completed\n", id)
    }
}

func main() {
    // Ограничиваем CPU для наглядности
    runtime.GOMAXPROCS(2)
    
    var wg sync.WaitGroup
    
    fmt.Printf("Starting with %d CPUs\n", runtime.GOMAXPROCS(0))
    fmt.Printf("Initial goroutines: %d\n", runtime.NumGoroutine())
    
    // Запускаем больше горутин, чем вмещают локальные очереди
    total := 600  // 2 CPU * 256 = 512 в локальных очередях + 88 в глобальной
    for i := 0; i < total; i++ {
        wg.Add(1)
        go worker(i, &wg)
        
        if i%100 == 0 {
            fmt.Printf("Created %d goroutines, total in system: %d\n", 
                i, runtime.NumGoroutine())
            time.Sleep(time.Millisecond) // Даем время на распределение
        }
    }
    
    fmt.Printf("All %d goroutines created, waiting...\n", total)
    fmt.Printf("Current goroutines: %d\n", runtime.NumGoroutine())
    
    wg.Wait()
    fmt.Println("All workers completed")
}
```

### Пример 2: Демонстрация балансировки

```go
package main

import (
    "fmt"
    "runtime"
    "sync"
    "sync/atomic"
    "time"
)

var (
    globalQueueCount int64
    localQueueCount  int64
)

func monitoredWorker(id int, wg *sync.WaitGroup) {
    defer wg.Done()
    
    // Имитация работы
    time.Sleep(time.Millisecond * 5)
    
    // Статистика о том, где выполнялась горутина
    if id%50 == 0 {
        fmt.Printf("Worker %d - Global: %d, Local: %d\n", 
            id, atomic.LoadInt64(&globalQueueCount), 
            atomic.LoadInt64(&localQueueCount))
    }
}

func main() {
    runtime.GOMAXPROCS(4)
    
    var wg sync.WaitGroup
    total := 1000
    
    // Создаем асимметричную нагрузку
    for i := 0; i < total; i++ {
        wg.Add(1)
        
        // Первые горутины создаем быстро (попадут в глобальную очередь)
        if i < 300 {
            atomic.AddInt64(&globalQueueCount, 1)
        } else {
            atomic.AddInt64(&localQueueCount, 1)
        }
        
        go monitoredWorker(i, &wg)
        
        // Имитируем неравномерное создание горутин
        if i < 300 {
            time.Sleep(time.Microsecond * 10)
        }
    }
    
    wg.Wait()
    fmt.Printf("Final stats - Global: %d, Local: %d\n",
        atomic.LoadInt64(&globalQueueCount), 
        atomic.LoadInt64(&localQueueCount))
}
```

## Взаимодействие с планировщиком

### Поиск работы планировщиком:

```go
// runtime/proc.go
func findrunnable() (gp *g, inheritTime bool) {
    // 1. Попытка взять из локальной очереди
    if gp, inheritTime := runqget(_p_); gp != nil {
        return gp, inheritTime
    }
    
    // 2. Попытка взять из глобальной очереди
    if sched.runqsize != 0 {
        lock(&sched.runqlock)
        gp := globrunqget(_p_, 0)
        unlock(&sched.runqlock)
        if gp != nil {
            return gp, false
        }
    }
    
    // 3. Work stealing у других P
    if gp := runqsteal(_p_, allp[enum.position()]); gp != nil {
        return gp, false
    }
    
    // 4. Сетевой poller и другие источники
    // ...
}
```

## Преимущества глобальной очереди

### 1. **Буфер для пиковой нагрузки**

```go
func peakLoadBuffer() {
    // Когда создается много горутин быстро:
    // - Локальные очереди заполняются
    // - Глобальная очередь принимает избыток
    // - Планировщик постепенно распределяет нагрузку
}
```

### 2. **Балансировка между процессорами**

```go
func loadBalancing() {
    // Свободные процессоры могут брать работу из глобальной очереди
    // Это обеспечивает лучшую утилизацию CPU
}
```

### 3. **Резервный источник работы**

```go
func backupWorkSource() {
    // Когда work stealing не находит работу у других P
    // Глобальная очередь служит последним источником горутин
}
```

## Ограничения глобальной очереди

### 1. **Блокировки доступа**

```go
func globalQueueLock() {
    // Доступ к глобальной очереди защищен мьютексом
    // Это может создавать contention при высокой нагрузке
    lock(&sched.runqlock)   // Блокировка
    // операции...
    unlock(&sched.runqlock) // Разблокировка
}
```

### 2. **Меньшая эффективность**

```go
func efficiencyComparison() {
    // Локальная очередь: ~0.2µs на операцию (без блокировок)
    // Глобальная очередь: ~1.0µs на операцию (с блокировками)
}
```

## Мониторинг глобальной очереди

### Пример мониторинга:

```go
package main

import (
    "fmt"
    "runtime"
    "time"
)

func globalQueueMonitor() {
    for {
        // В реальном коде здесь был бы доступ к sched.runqsize
        // через internal пакеты или debug информацию
        
        goroutines := runtime.NumGoroutine()
        cpus := runtime.GOMAXPROCS(0)
        
        fmt.Printf("Goroutines: %d, CPUs: %d\n", goroutines, cpus)
        
        // Эмпирическая оценка размера глобальной очереди
        if goroutines > cpus * 256 {
            estimatedGlobal := goroutines - cpus * 256
            fmt.Printf("Estimated global queue size: %d\n", estimatedGlobal)
        }
        
        time.Sleep(2 * time.Second)
    }
}

func main() {
    go globalQueueMonitor()
    
    // Создаем нагрузку
    for i := 0; i < 2000; i++ {
        go func(id int) {
            time.Sleep(time.Minute)
        }(i)
        time.Sleep(time.Millisecond)
    }
    
    select {}
}
```

## Реальный сценарий: Обработка запросов

```go
package main

import (
    "fmt"
    "net/http"
    "runtime"
    "time"
)

func burstHandler(w http.ResponseWriter, r *http.Request) {
    // Имитация всплеска нагрузки
    for i := 0; i < 100; i++ {
        go func(reqID int) {
            // Обработка части запроса
            time.Sleep(time.Millisecond * 10)
        }(i)
    }
    
    fmt.Fprintf(w, "Burst processed - Goroutines: %d", 
        runtime.NumGoroutine())
}

func main() {
    http.HandleFunc("/burst", burstHandler)
    
    fmt.Println("Server started on :8080")
    fmt.Println("Global queue will handle burst loads")
    
    http.ListenAndServe(":8080", nil)
}
```

## Итог

**Глобальная очередь в планировщике Go — это:**

1. **🔄 Буфер переполнения** для локальных очередей
2. **⚖️ Механизм балансировки** нагрузки между процессорами  
3. **🆘 Резервный источник** работы для свободных P
4. **🔒 Защищенная мьютексом** структура (меньшая производительность)
5. **📈 Амортизатор пиковых нагрузок**

Глобальная очередь работает в тесной связке с локальными очередями, обеспечивая устойчивость системы при резких всплесках нагрузки и эффективное распределение работы между всеми доступными процессорами.
# 9. Wait очередь
Отличный вопрос! **Wait очередь** — это важный механизм в планировщике Go для управления заблокированными горутинами.

## Что такое wait очередь?

**Wait очередь** — это структура данных, которая хранит горутины, ожидающие на различных примитивах синхронизации:
- **Каналы** (channel operations)
- **Мьютексы** (mutexes) 
- **Условные переменные** (condition variables)
- **Таймеры** (timers)
- **Сетевые операции** (network I/O)

## Структуры данных wait очередей

### 1. **sudog - структура ожидающей горутины**

```go
// runtime/runtime2.go
type sudog struct {
    // Горутина, которая ожидает
    g *g
    
    // Ссылка на следующий sudog в очереди
    next *sudog
    prev *sudog
    
    // Объект синхронизации (канал, мьютекс и т.д.)
    elem unsafe.Pointer // data element
    
    // Примитив синхронизации
    c        *hchan     // канал
    waitlink *sudog     // wait queue for semaphore
    waittail *sudog     
    
    // Таймеры
    t        *timer
    
    // Для select
    isSelect bool
    success  bool
    
    // Защита от повторного использования
    releasetime int64
}
```

## Типы wait очередей

### 1. **Wait очередь каналов**

```go
// runtime/chan.go
type hchan struct {
    // Данные канала
    qcount   uint           // общее количество элементов в очереди
    dataqsiz uint           // размер буфера канала
    buf      unsafe.Pointer // указатель на буфер
    elemsize uint16
    elemtype *_type // тип элемента
    
    // Очереди ожидания
    sendq    waitq  // очередь отправляющих горутин
    recvq    waitq  // очередь получающих горутин
    
    lock mutex
}

type waitq struct {
    first *sudog  // голова очереди
    last  *sudog  // хвост очереди
}
```

### 2. **Wait очередь мьютексов**

```go
// runtime/sync.go
type mutex struct {
    // Битовая маска состояния
    state int32
    
    // Очередь ожидания
    sema  uint32
}

// Очередь ожидания для sync.Mutex
func sync_runtime_SemacquireMutex(addr *uint32, lifo bool, skipframes int) {
    // Добавление в очередь ожидания мьютекса
}
```

## Механизм работы wait очередей

### 1. **Блокировка на канале (send)**

```go
// runtime/chan.go
func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool {
    lock(&c.lock)
    
    // Если есть ожидающие получатели - разбудить первого
    if sg := c.recvq.dequeue(); sg != nil {
        // Прямая передача данных получателю
        unlock(&c.lock)
        return true
    }
    
    // Если буфер полный - заблокироваться
    if c.qcount >= c.dataqsiz {
        // Создание sudog и добавление в очередь отправки
        gp := getg()
        mysg := acquireSudog()
        mysg.g = gp
        mysg.elem = ep
        mysg.c = c
        gp.waiting = mysg
        
        // Добавление в очередь отправки
        c.sendq.enqueue(mysg)
        
        // Переход в состояние ожидания
        gopark(chanparkcommit, unsafe.Pointer(&c.lock), waitReasonChanSend, traceEvGoBlockSend, 2)
        
        // Пробуждение
        releaseSudog(mysg)
        return true
    }
    
    unlock(&c.lock)
    return true
}
```

### 2. **Блокировка на канале (receive)**

```go
func chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool) {
    lock(&c.lock)
    
    // Если есть ожидающие отправители - разбудить первого
    if sg := c.sendq.dequeue(); sg != nil {
        // Прямой прием данных от отправителя
        unlock(&c.lock)
        return true, true
    }
    
    // Если буфер пустой - заблокироваться
    if c.qcount == 0 {
        gp := getg()
        mysg := acquireSudog()
        mysg.g = gp
        mysg.elem = ep
        mysg.c = c
        gp.waiting = mysg
        
        // Добавление в очередь получения
        c.recvq.enqueue(mysg)
        
        // Переход в состояние ожидания
        gopark(chanparkcommit, unsafe.Pointer(&c.lock), waitReasonChanReceive, traceEvGoBlockRecv, 2)
        
        // Пробуждение
        releaseSudog(mysg)
        return true, false
    }
    
    unlock(&c.lock)
    return true, true
}
```

## Практическая демонстрация

### Пример 1: Wait очередь канала

```go
package main

import (
    "fmt"
    "runtime"
    "sync"
    "time"
)

func demonstrateChannelWaitQueue() {
    ch := make(chan int, 2) // Буфер размером 2
    
    var wg sync.WaitGroup
    
    // Producer - заполняет буфер и блокируется
    wg.Add(1)
    go func() {
        defer wg.Done()
        
        fmt.Println("Producer: sending 1, 2, 3...")
        ch <- 1 // Проходит
        ch <- 2 // Проходит
        fmt.Println("Producer: buffer full, blocking on send 3...")
        ch <- 3 // БЛОКИРОВКА - попадает в sendq wait queue
        fmt.Println("Producer: unblocked, sent 3")
    }()
    
    time.Sleep(100 * time.Millisecond)
    
    // Consumer - освобождает место в буфере
    wg.Add(1)
    go func() {
        defer wg.Done()
        
        time.Sleep(500 * time.Millisecond) // Даем producer'у заблокироваться
        
        fmt.Println("Consumer: receiving...")
        fmt.Println("Received:", <-ch) // Освобождает место и будит producer'a
    }()
    
    wg.Wait()
    fmt.Printf("Active goroutines: %d\n", runtime.NumGoroutine())
}
```

### Пример 2: Wait очередь мьютекса

```go
package main

import (
    "fmt"
    "runtime"
    "sync"
    "time"
)

func demonstrateMutexWaitQueue() {
    var mu sync.Mutex
    var wg sync.WaitGroup
    
    resource := "initial"
    
    // Первая горутина захватывает мьютекс надолго
    wg.Add(1)
    go func() {
        defer wg.Done()
        
        fmt.Println("Goroutine 1: acquiring mutex...")
        mu.Lock()
        fmt.Println("Goroutine 1: mutex acquired, working...")
        
        resource = "modified by goroutine 1"
        time.Sleep(2 * time.Second) // Долгая работа
        
        mu.Unlock()
        fmt.Println("Goroutine 1: mutex released")
    }()
    
    time.Sleep(100 * time.Millisecond) // Даем первой горутине захватить мьютекс
    
    // Вторая горутина блокируется на мьютексе
    wg.Add(1)
    go func() {
        defer wg.Done()
        
        fmt.Println("Goroutine 2: trying to acquire mutex...")
        mu.Lock() // БЛОКИРОВКА - попадает в wait queue мьютекса
        fmt.Println("Goroutine 2: mutex acquired!")
        
        resource = "modified by goroutine 2"
        mu.Unlock()
        fmt.Println("Goroutine 2: mutex released")
    }()
    
    // Третья горутина также блокируется
    wg.Add(1)
    go func() {
        defer wg.Done()
        
        fmt.Println("Goroutine 3: trying to acquire mutex...")
        mu.Lock() // БЛОКИРОВКА - попадает в wait queue мьютекса
        fmt.Println("Goroutine 3: mutex acquired!")
        
        resource = "modified by goroutine 3"
        mu.Unlock()
        fmt.Println("Goroutine 3: mutex released")
    }()
    
    wg.Wait()
    fmt.Println("Final resource:", resource)
    fmt.Printf("Active goroutines: %d\n", runtime.NumGoroutine())
}
```

### Пример 3: Мониторинг wait очередей

```go
package main

import (
    "fmt"
    "runtime"
    "sync"
    "time"
)

func monitorWaitQueues() {
    ch := make(chan int) // Небуферизованный канал
    var mu sync.Mutex
    
    var wg sync.WaitGroup
    
    // Монитор горутин
    go func() {
        for {
            fmt.Printf("Goroutines: %d (waiting in queues)\n", 
                runtime.NumGoroutine())
            time.Sleep(500 * time.Millisecond)
        }
    }()
    
    // Блокировка на канале
    wg.Add(2)
    go func() {
        defer wg.Done()
        fmt.Println("Sender: blocking on channel send...")
        ch <- 42 // Блокировка в sendq
        fmt.Println("Sender: unblocked")
    }()
    
    go func() {
        defer wg.Done()
        fmt.Println("Receiver: blocking on channel receive...")
        <-ch // Блокировка в recvq
        fmt.Println("Receiver: unblocked")
    }()
    
    time.Sleep(1 * time.Second)
    
    // Блокировка на мьютексе
    wg.Add(3)
    mu.Lock()
    
    for i := 0; i < 3; i++ {
        go func(id int) {
            defer wg.Done()
            fmt.Printf("Mutex waiter %d: blocking...\n", id)
            mu.Lock() // Блокировка в wait queue мьютекса
            fmt.Printf("Mutex waiter %d: acquired mutex\n", id)
            mu.Unlock()
        }(i)
    }
    
    time.Sleep(1 * time.Second)
    fmt.Println("Releasing mutex...")
    mu.Unlock()
    
    wg.Wait()
}
```

## Механизм пробуждения

### Функция gopark и goready

```go
// runtime/proc.go
func gopark(unlockf func(*g, unsafe.Pointer) bool, lock unsafe.Pointer, reason waitReason, traceEv byte, traceskip int) {
    // Переводит горутину в состояние ожидания
    mp := acquirem()
    releasem(mp)
    
    // Сохранение контекста
    mg := getg()
    mg.m.waitlock = lock
    mg.m.waitunlockf = unlockf
    
    // Смена состояния
    casgstatus(mg, _Grunning, _Gwaiting)
    
    // Вызов планировщика
    schedule()
}

func goready(gp *g, traceskip int) {
    // Пробуждает горутину
    systemstack(func() {
        ready(gp, traceskip, true)
    })
}
```

## Особенности wait очередей

### 1. **FIFO порядок** (обычно)

```go
// В большинстве случаев - First-In-First-Out
// Первая заблокированная горутина пробуждается первой
```

### 2. **Прямая передача данных**

```go
// При операциях с каналами возможна прямая передача
// между отправителем и получателем минуя буфер
```

### 3. **Интеграция с netpoller**

```go
// Сетевые операции используют отдельный механизм
// через netpoller, но также используют wait queues
```

## Реальный сценарий: Producer-Consumer

```go
package main

import (
    "fmt"
    "sync"
    "time"
)

func producerConsumerWithWaitQueues() {
    const (
        numProducers = 3
        numConsumers = 2
        bufferSize   = 5
    )
    
    ch := make(chan int, bufferSize)
    var wg sync.WaitGroup
    
    // Producers
    for i := 0; i < numProducers; i++ {
        wg.Add(1)
        go func(id int) {
            defer wg.Done()
            
            for j := 0; j < 10; j++ {
                item := id*100 + j
                fmt.Printf("Producer %d: sending %d\n", id, item)
                ch <- item
                fmt.Printf("Producer %d: sent %d\n", id, item)
                time.Sleep(50 * time.Millisecond)
            }
        }(i)
    }
    
    // Consumers
    for i := 0; i < numConsumers; i++ {
        wg.Add(1)
        go func(id int) {
            defer wg.Done()
            
            for j := 0; j < 15; j++ {
                time.Sleep(100 * time.Millisecond) // Медленные consumers
                item := <-ch
                fmt.Printf("Consumer %d: received %d\n", id, item)
            }
        }(i)
    }
    
    wg.Wait()
    close(ch)
    fmt.Println("All done!")
}
```

## Итог

**Wait очереди в Go — это:**

1. **⏳ Структуры ожидания** для заблокированных горутин
2. **📋 sudog элементы** представляют ждущие горутины
3. **🔗 Связанные списки** для каждого примитива синхронизации
4. **🎯 Интеграция с планировщиком** через gopark/goready
5. **⚡ Эффективное пробуждение** в правильном порядке

Wait очереди обеспечивают корректную работу примитивов синхронизации и эффективное управление тысячами ожидающих горутин без потребления CPU ресурсов.
# 10. Очередь net pull

# 11. Рантайм го

# 12. Сборщик мусора го

# 13. За счет чего го позволяет работать с большим кол-вом сетевых соединений

# 14. Каналы

# 15. Что будет если записать в nil канал

# 16. Что будет если прочитать в nil канал

# 17. deadlock

# 18. передача из одной горутины в другую через канал

# 19. мьютексы

# 20. как реализован буфер в го каналах

# 21. атомики

# 22. wait group

# 23. интерфейсы

# 24. утинная типизация

# 25. из чего состоят интерфейс

# 26. память в го

# 27. отличие слайса от мапы

# 28. длина слайса и емкость

# 29. как меняется длина слайса

# 30. как внутри реализована мапа

# 31. эвакуация данных

# 32. сборка мусора

# 33. write барьер

# 34. тесты в го

# 35. аргументы команды go test

# 36. моки

# 37. профилирование го программ

# 38.  рекурсивной блокировки

# 39. как увеличивается cap slice ------------------------------------

# 40. сложность операций со слайс
